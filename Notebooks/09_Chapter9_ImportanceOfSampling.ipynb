{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9      \n",
    "# Importance of Sampling\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Sampling is a fundamental process in the collection and analysis of data. Sampling is important because we can almost never collect data on an entire population. Further, sampling must be randomized to preclude biases.    \n",
    "\n",
    "Some key points to keep in mind about sampling:   \n",
    "- An understanding of sampling is essential to ensure that the analysis performed is representative of the entire population.   \n",
    "- You will use inferences on the sample to say something about the population.   \n",
    "- The sample must be randomly drawn from the population.   \n",
    "\n",
    "Let's look at some examples of sampling.   \n",
    "\n",
    "| Use Case | Sample | Population |\n",
    "|---|---|---|\n",
    "| A/B Testing | The users we show either web sites A or B | All possible users, past present and future|   \n",
    "|World Cup Soccer | 32 teams which qualify in one season | All national teams in past, present and future years|   \n",
    "|Average height of data science students | Students in a data science class | All students taking data science classes world wide|   \n",
    "|Tolerances of a manufactured part | Samples taken from production lines | All parts manufactured in the past, present and future |   \n",
    "|Numbers of a species in a habitat |Population counts from sampled habitats |All possible habitats in the past, present and future |   \n",
    "\n",
    "Notice, that in several cases it is not only impractical but impossible to collect data from the entire population. Hence, we nearly always work with samples, rather than the entire population. \n",
    "\n",
    "### Importance of random sampling   \n",
    "\n",
    "Essentially all statistical methods rely on the use of **randomized unbiased samples**. Failure to use such samples violates many of the key assumptions of statistical models. Thus, an understanding of the proper use of sampling methods is essential to performing statistical inference.   \n",
    "\n",
    "Further, most commonly used machine learning algorithms assume that training data are **unbiased** and **independent identically distributed (iid)**. These conditions will only be met if the training data sample is randomized. Otherwise, the training data will be biased and not represent the underlying process distribution.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 9-1:** As an example of the problems that can arise from poor sampling consider the following. A physical retailer wishes to better understand the types of customers in areas where their stores are located. People are hired to hand coupons to customers entering the stores. The coupons are all worth 5 dollars off a subsequent purchase. The coupons are activated if the customer goes to a web site and spends approximately 20 minutes completing a questionnaire. Coupons are distributed during regular business hours Monday to Friday. There are several sources of biases inherent in this sampling method. Discus the sources of these biases, keeping in mind there are several and the scope of your answer is open-ended. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:** Possible biases include the following:      \n",
    "> 1. The coupons are only distributed to customers who shop on weekdays, so there is no sampling of weekend shoppers.     \n",
    "> 2. Data is only acquired from customers willing to spend 20 min filling out a web form.     \n",
    "> 3. There is no sampling of prospective customers at all.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The sampling distributions   \n",
    "\n",
    "We already know that in most practical situations we will only be able to sample from the true data-generating process of the population. This sampling is done from an unknown **population distribution**, $\\mathcal{F}$. Any statistic we compute for the generating process is actually based on a sample.   \n",
    "\n",
    "We would like to compute a statistic of interest from this distribution of the entire population, $s(\\mathcal{F})$. But as we have seen already, we can only compute the statistic from an approximation of this distribution, $\\hat{\\mathcal{F}}$. Any statistic is actually computed from an estimate, $s(\\hat{\\mathcal{F}})$.   \n",
    "\n",
    "If we continue to take random samples from the population and compute estimates of a statistic, we say the statistic has a **sampling distribution**. This hypothetical concept is a foundation of **frequentist statistics**. In the frequentist world, statistical inferences are performed on the sampling distribution. Random sampling of the population distribution is required so that the statistical estimates are not biased by the sampling process. The assumptions of frequentist statistics are built on the idea of being able to randomly resample from the population distribution and recompute a statistic. Given a sample, $\\hat{\\mathcal{F}}$, we can compute an estimate of a statistic, s, from that sample $s(\\hat{\\mathcal{F}})$. We the value of the statistic is only an estimate since it is computed from a sample, $\\hat{\\mathcal{F}}$, not the population.        \n",
    "\n",
    "A conceptual (thought) experiment is shown in the figure below. If we continue to draw new **random and independent samples** from the population, the distribution of the resulting values of the statistic estimates, $s(\\hat{\\mathcal{F}})$, are then characterized by the **sampling distribution**.       \n",
    "\n",
    "<img src=\"../images/SamplingDistribuion.png\" alt=\"Drawing\" style=\"width:700px; height:400px\"/>\n",
    "<center>Repeated sampling and the sampling distribution</center>     \n",
    "\n",
    "Consider the example of an engineer who needs to characterize some statistics describing the noise in an electronic system, such as the signal-to-noise ratio. The engineer might use instruments to measure (sample) the noise. We are denoting the sample from this single experiment as, $\\hat{\\mathcal{F}}$, from which the engineer computes a statistic, $s(\\hat{\\mathcal{F}})$. The engineer could run multiple experiments, each of which provides a new sample and a new estimate of the statistic, $s(\\hat{\\mathcal{F}})$. The variability of these estimates is then characterized by the sampling distribution.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling and the law of large numbers\n",
    "\n",
    "The **law of large numbers** is a theorem that states that **statistics of independent random samples converge to the population values as more samples are used**. The amazing fact is that the law of large numbers applies to any statistic computed from a sample of a population!   \n",
    "\n",
    "The law of large numbers is foundational to statistics. We rely on the law of large numbers whenever we work with samples of data. We can safely assume that **larger samples are more representative of the population we are sampling**. This theorem is the foundation of not only sampling theory, but modern computational methods including, simulation, bootstrap resampling, and Monte Carlo methods. If the real world did not follow this theorem much of statistics, to say nothing of science and technology, would fail badly!    \n",
    "\n",
    "Given this great importance, it should not be surprising that the law of large numbers has a long history. Jacob Bernoulli posthumously published the first proof of the Binomial distribution in 1713. In fact, the law of large numbers is sometimes referred to as **Bernoulli's theorem**. A more general proof was published by Poisson in 1837.    \n",
    "\n",
    "Let's consider a simple example of computing a statistic, the mean. We denote our sample estimate of the mean, $\\bar{X}$, and the unknowable population mean or population parameter, $\\mu$. We can write the estimator for this statistic mathematically as:\n",
    "\n",
    "$$Let\\ \\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i$$\n",
    "\n",
    "Then by the law of Large Numbers:\n",
    "\n",
    "$$ \\bar{X} \\rightarrow E(X) = \\mu\\\\\n",
    "as\\\\\n",
    "n \\rightarrow \\infty$$\n",
    "\n",
    "Execute the code in the cell below to import the required packages for the examples and exercises. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's think about a simple example. The mean of 50 coin flips (0,1)=(T,H) is usually further away from the true mean of 0.5 than 5,000 coin flips. The code in the cell below computes and then samples a population of 1,000,000 Bernoulli trials with $p=0.5$. Run this code and  examine the results. Does the expected value, or mean converge to 0.5 as `n` increases?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr.seed(3457)\n",
    "n = 1\n",
    "p = 0.5\n",
    "size = 1000000\n",
    "# Create a large binomial distributed population. \n",
    "pop = pd.DataFrame({'var':nr.binomial(n, p, size)}) \n",
    "# Sample the population for different sizes and compute the mean\n",
    "sample_size = [5, 50, 500, 5000]\n",
    "out = [pop.sample(n = x).mean(axis = 0)[0] for x in sample_size] \n",
    "#for n,x in zip(sample_size,out): print(\"%.0f  %.2f\" %(n, x))\n",
    "pd.DataFrame({'Sample Size':sample_size, 'Emperical Mean':out})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 9-2:** You will use the series of Bernoulli trials just computed to examine the convergence of the estimated probability, $p$. This computational experiment is equivalent to determining if a long series of flips of a fair coin converge to $p = 0.5$. Now do the following:   \n",
    "> 1. Create and execute code to compute the running mean of the realizations of a series of Bernoulli trials as the sample size increases from 1 to 5000. Your result should be in the form of a data frame with one column containg the number of samples and the other the running mean.   \n",
    "> 2. Create a function to scatter plot the estimate of $p$ vs. the running number of samples. Include a distinctive horizontal line at the theoretical value of $p$. Make sure your plot is correctly labeled. Run your funtion twice; once for 1 to 500 realizations, and once for 1 to 5000 realizations. \n",
    "> Execute your code, which may take some time.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Do the estimates of $p$ appear to converge to the known value and the number of realizations increases consistent with the law of large numbers? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard error and convergence for a Normal distribution\n",
    "\n",
    "As we sample from a Normal distribution, the mean of the sample will converge to the population mean and the sample standard deviation will converge to the population standard deviation as the number of samples increases. This behavior is expected from the law of large numbers. \n",
    "\n",
    "\n",
    "But, what can we say about the expected error of the mean estimate as the number of samples increases? This measure is known as the **standard error** of the sample mean. As a result of the **Central Limit Theorem** the standard error is defined:\n",
    "\n",
    "$$se = \\pm \\frac{sd}{\\sqrt{(n)}}$$\n",
    "\n",
    "This relationship has significant implications for sampling. The standard error decreases as the square root of $n$. For example, if you wish to halve the error, you will need to sample four times as many values.   \n",
    "\n",
    "For the mean estimate, $\\mu$, we can then define the uncertainty in terms of **confidence intervals**. The Central Limit theorem and confidence intervals are discussed in depth in Chapter 12. For now, we state that for Normally distributed values the 95% confidence interval is:\n",
    "\n",
    "$$CI_{95} =\\mu \\pm 1.96\\ se$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 9-3:** To explore these concepts, you will do the following:\n",
    "> 1. Compute the mean using 1 to 10000 samples of from a standard Normal distribution; $\\mathcal{N(1,0)}$.   \n",
    "> 2. Create a data frame with the following columns; i) the number of samples, ii) the running mean estimates, iii) the theoretical upper confidence interval, iv) the running estimated upper confidence interval, v) the theoretical lower confidence interval, iv) the running estimated lower confidence interval.   \n",
    "> 3. Create a scatter plot of the mean estimates. Include on this plot distinctive lines showing the upper and lower theoretical confidence intervals and the population (theoretical) mean. Include a legend and other required annotation on your plot.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put you code below   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Does your plot show convergence to the population mean as you would expect from the law of large numbers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Strategies\n",
    "\n",
    "There are a great number of possible sampling methods. In this lesson, we will focus on some of the most commonly used methods. At the conclusion of this lesson, you should be able to apply (or avoid) these sampling strategies: \n",
    "\n",
    "- **Bernoulli sampling**, a foundation of random sampling. \n",
    "- **Stratified sampling**, when groups with different characteristics must be sampled.\n",
    "- **Cluster sampling**, to reduce cost of sampling. \n",
    "- **Systematic sampling and convenience sampling**, a slippery slope.\n",
    "\n",
    "### Bernoulli Sampling\n",
    "\n",
    "**Bernoulli sampling** is a widely used foundational random sampling strategy. Bernoulli sampling has the following properties:\n",
    "\n",
    "- A **single random sample** of the population is created.   \n",
    "- A particular value in the population is selected based on the outcome of a Bernoulli trial with a fixed probability of success, $p$.    \n",
    "- The sample has a **fixed or predetermined size**.   \n",
    "\n",
    "Consider an example of a company that sells a product by weight. The company wants to ensure the quality of the packaging process so that few packages are underweight. But, it is impractical to empty and weigh the contents of every package. Instead, Bernoulli randomly sampled packages from the production line are emptied and the contents weighed. Statistical inferences are then made based on the sample to determine if the packaging process meets the required standards.  \n",
    "\n",
    "Let's try an example with some synthetic data. The code below creates a data frame with 2,000 random samples from the standard Normal distribution. These realizations are generated with [numpy.random.normal](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.normal.html). Next, these realizations are randomly divided into 4 groups using [numpy.random.choice](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.choice.html). Notice that the probability of a sample being in one of the groups is not uniform, and sums to 1.0.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr.seed(345)\n",
    "population_size = 10000\n",
    "data = pd.DataFrame({\"var\":nr.normal(size = population_size), \n",
    "                     \"group\":nr.choice(range(4), size= population_size, p = [0.1,0.3,0.4,0.2])})\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You know that the population of 2000 values was sampled from the standard Normal distribution. Therefore, the mean of each group should be close to 0.0. To find out, the code below does the following:\n",
    "1. The sample is divided into 4 groups.   \n",
    "2. Summary statistics are computed.\n",
    "3. A data frame is created with one column containing the count of the numbers, the mean, standard error, and confidence intervals for each group.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_mean(dat):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    groups = dat.groupby('group') # Create the groups\n",
    "    n_samples = groups.size()\n",
    "    se = np.sqrt(np.divide(groups.aggregate(np.var).loc[:, 'var'], np.sqrt(n_samples)))\n",
    "    means = groups.aggregate(np.mean).loc[:, 'var']\n",
    "    ## Create a data frame with the counts and the means of the groups\n",
    "    return pd.DataFrame({'Count': n_samples, \n",
    "                        'Mean': means,\n",
    "                        'SE': se,\n",
    "                        'Upper_CI': np.add(means, 1.96 * se),\n",
    "                        'Lower_CI': np.add(means, -1.96 * se)})\n",
    "count_mean(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the means for each group all all close to, but not exactly, 0.0. \n",
    "\n",
    "> **Exercise 9-4:** The foregoing mean estimates look reasonable for the full population of 20000. But, what happens when the population is sampled and the mean is computed from the sample? To find out, do the following:\n",
    "> 1. Create a Bernoulli sample of the population with sampling probability, $p = 0.04$.  You can use [numpy.random.choice](https://docs.scipy.org/doc//numpy-1.10.4/reference/generated/numpy.random.choice.html) for the sampling.   \n",
    "> 2. Compute and display the mean by group for the Bernoulli sample. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Examine these results and answer these questions:\n",
    "> 1. Given the number of samples per group do the standard errors and confidence intervals seem consistent with the number of samples in each group?  \n",
    "> 2. Does Bernoulli sampling seem optimal when data falls into distinct groups, and why?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**     \n",
    "> 1.            \n",
    "> 2.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified Sampling\n",
    "\n",
    "You might well ask, why are we concerned with sampling grouped data? Group data is quite common in applications. Just a few examples include     \n",
    "1. Pooling opinion by county and income group, where income groups and counties have significant differences in population.    \n",
    "2. Testing a drug which may have different effectiveness by sex and ethnic group.   \n",
    "3. Spectral characteristics of stars by type.  \n",
    "\n",
    "What is a better strategy for sampling grouped or stratified data? **Stratified sampling** strategies are used when data are organized in **strata**. The idea is simple: independently sample an equal number of cases from each group. The simplest version of stratified sampling creates an **equal-size Bernoulli sample** from each strata.\n",
    "\n",
    "In many cases, you may need to create nested samples. For example, a top-level sample can be grouped by zip code, a geographic strata. Within each zip code, people are then sampled by income bracket strata. The opposite stratification could also be used. Regardless of the exact structure of the hierarchy equal sized Bernoulli samples are collected at the lowest level.   \n",
    "\n",
    "The code below samples our population uniformly, by group. The mean, standard error, and confidence intervals are then computed for each group.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.01\n",
    "def stratify(dat, p):\n",
    "    groups = dat.groupby('group') # Create the groups\n",
    "    nums = min(groups.size()) # Find the size of the smallest group\n",
    "    num = int(p * dat.shape[0]) # Compute the desired number of samples per group\n",
    "    if num <= nums: \n",
    "        ## If sufficient group size, sample each group.\n",
    "        ## We drop the unneeded index level and return, \n",
    "        ## which leaves a data frame with just the original row index. \n",
    "        return groups.apply(lambda x: x.sample(n=num)).droplevel('group')\n",
    "    else: # Oops. p is to large and our groups cannot accommodate the choice of p.\n",
    "        pmax = nums / dat.shape[0]\n",
    "        print('The maximum value of p = ' + str(pmax))\n",
    "stratified = stratify(data, p)\n",
    "count_mean(stratified)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 9-5:** Answer the following quesitons;      \n",
    "> 1. Are the number of samples identical for each group?         \n",
    "> 2. Are the standard errors and confidence intervals similar for each group?    \n",
    "> 3. How do these standard errors and confidence intervals compare those for the groups with non-stratified samples?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**      \n",
    "> 1.           \n",
    "> 2.        \n",
    "> 3.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Sampling\n",
    "\n",
    "When sampling is expensive a strategy is required to reduce the cost, yet still keep randomization Some examples of data which is expensive to collect includes:\n",
    "\n",
    "- Surveys of customers at a chain of stores.\n",
    "- Door-to-door survey of homeowners.\n",
    "- Sampling wildlife populations in a dispersed habitat. \n",
    "\n",
    "In these cases, the population can be divided into randomly selected clusters. The process of cluster sampling follows these steps:\n",
    "\n",
    "- Define the clusters for the population.\n",
    "- Randomly select the required number of clusters.\n",
    "- Sample from the selected clusters. Typically Bernoulli sampling or some other random sampling is used. \n",
    "- Optionally, stratify the sample within each cluster.\n",
    "\n",
    "As an example, you can select a few store locations and Bernoulli sample customers at these locations.\n",
    "\n",
    "The code in the cell below divides a population into 10 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First compute the clusters\n",
    "num_clusters = 10\n",
    "num_vals = 1000\n",
    "## Create a data frame with randomly sampled cluster numbers\n",
    "clusters = pd.DataFrame({'group': range(num_clusters)}).sample(n = num_vals, replace = True)\n",
    "## Add a column to the data frame with Normally distributed values\n",
    "clusters.loc[:, 'var'] = nr.normal(size = num_vals)\n",
    "clusters.head(10) # Print the head to get a feel for these data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 9-6:** With these clusters defined you will now perform some sampling to gain a bit of understanding of these methods. Perform the following steps:  \n",
    "> 1. Create and execute code to find and display the count of samples in each of the clusters. \n",
    "> 2. Randomly sample 3 of the 10 clusters using [numpy.random.choice](https://docs.scipy.org/doc//numpy-1.10.4/reference/generated/numpy.random.choice.html), and display the resulting cluster sampled data frame. **Note:** if you do not set a seed, the clusters selected may be different each time your run your code. \n",
    "> 3. Bernoulli samples each of the clusters selected with `p = 0.2`, and then stratifies these samples by group.\n",
    "> 4. Compute and display the mean, standard error, and confidence intervals of the strata for each of the three clusters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Examine your results and answer these questions:   \n",
    "> 1. Are the number of samples in each stratum of the clusters approximately equal?  \n",
    "> 2. Do the mean estimates, standard errors, and confidence intervals appear similar?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**        \n",
    "> 1.                  \n",
    "> 2.         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Systematic Sampling\n",
    "\n",
    "**Convenience and systematic sampling** are a slippery slope toward biased inferences. These sampling methods lack randomization and the samples created are generally not representative of the population. \n",
    "\n",
    "As the name implies, convenience sampling selects the cases that are easiest to obtain. A commonly cited example of convenience sampling is known as **database sampling**. For example, the first N rows resulting from a query database query are used for an analysis. There is no reason to believe that such a sample is representative of a population in any way. \n",
    "\n",
    "Systematic sampling applies some convenient, but not random, sampling method. For example, every k-th case of the population is selected. As you can imagine, this is not a random sampling method, but rather a case of convenience sampling. \n",
    "\n",
    "> **WARNING:** Systematic sampling is a form of convenience sampling. Convenience sampling inevitably leads to incorrect inferences!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo-Random Number Generation  \n",
    "\n",
    "By now, you likely have an impression that **pseudo-random number generation** is vital to computational statistics. In fact, many areas of computational statistics and machine learning rely on our ability to efficiently and correctly generate these sequences. We will now provide an outline of the basic ideas.    \n",
    "\n",
    "The term, pseudo-random indicates that the algorithms we use do not actually generate truly random numbers, but rather a seemingly random sequence. This pseudo-random sequence is not infinite, but is said to have **cycles**. Good pseudo-random number generators have very long cycles.   \n",
    "\n",
    "A pseudo-random number generator is actually a deterministic algorithm which creates an apparently random sequence of numbers. This sequence continues for the length of a cycle. These algorithms do so by a clever sequence of mathematical operations on a set of binary numbers. Long cycles arise from using large prime numbers as multipliers in the generator.   \n",
    "\n",
    "Creation of pseudo-random number generator algorithms has a long and fraught history, predating the computer era. For example, the German [Enigma code machines](https://en.wikipedia.org/wiki/Enigma_machine#:~:text=The%20Enigma%20machine%20is%20an,branches%20of%20the%20German%20military.) of the Second World War used a set of mechanical rotors to create pseudo-random numbers. The encoded characters of a message were multiplied by the pseudo-random numbers. Decoding the resulting cipher required knowing the seed used to start the pseudo-random sequence. The Bletchley Park code breakers, including Allen Touring, exploited the relatively short cycles of the mechanical pseudo random number generation to decrypt these ciphers.  \n",
    "\n",
    "In the mainframe era, pseudo-random number generation suffered from short cycles. The cycles were often unexpectedly short. In many cases, the early algorithms had shorter cycles than was known at the time. This situation resulted in biases from unexpectedly repeating pseudo-random numbers.   \n",
    "\n",
    "It took until the late 20th Century for reliable long-cycle pseudo-random number generator algorithms to be developed. The [Mersenne Twister algorithm](https://en.wikipedia.org/wiki/Mersenne_Twister) developed by Matsumoto and Nishimura (1997) is now used in virtually all open source statistics and machine learning packages, as well most commercial products. The name comes from a special class of prime numbers known as Mersenne primes.    \n",
    "\n",
    "The [numpy.random.RandomState class](https://numpy.org/doc/1.13/reference/generated/numpy.random.RandomState.html), which is used widely in Python analytical packages, uses the Mersenne twister algorithm. The methods of this class perform the transformations for the specified probability distribution.   \n",
    "\n",
    "Pseudo-random number generator algorithms create draws from a Uniform distribution, at least within a cycle. To generate draws from other distributions, the uniformly distributed values are transformed using the inverse of the target cumulative probability function.    \n",
    "\n",
    "Since the algorithms are actually deterministic, the sequence of values can be repeated introducing a **seed value**. If you are creating test cases for statistical algorithms that rely on pseudo-random number generation, you must supply a seed for your test. Otherwise, the test results are not reproducible. If no seed is specified ‘entropy’ is achieved by queries on counters used by the operating system. In this case, the pseudo-random sequence generated will depend on the state of these counters.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A few more thoughts on sampling\n",
    "\n",
    "There are many practical aspects of sampling. Here are a few key points to keep in mind when performing sampling. The takeaway here is to create a random sampling plan in advance and stick to your plan. \n",
    "\n",
    "- Random sampling is essential to the underlying assumptions of statistical inference. \n",
    "- Whenever you are planning to sample data, make sure you have a clear sampling plan. \n",
    "- Know the number of clusters, strata, and samples in advance.\n",
    "- Don’t stop sampling when the desired result is achieved: e.g. error measure! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lesson, you have explored the concepts of sampling and how sampling relates to the law of large numbers. Nearly all data statisticians and data scientists work with are sampled from larger, unknown, populations. It is therefore important to have an understanding of sampling methods and the convergence of statistical estimates. \n",
    "\n",
    "Specifically in this lesson, you have done the following:\n",
    "- Estimated sample statistics converge to the population statistics as the sample size grows. This convergence is referred to as *The Law of Large Numbers*.\n",
    "- Used Bernoulli sampling to generate randomized samples of a population.\n",
    "- Applied stratified sampling to populations with unequal numbers of members. \n",
    "- Used cluster sampling to reduce the number of samples required for cases where data collection is expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography \n",
    "\n",
    "#### Copyright 2020, 2021, 2022, 2023 Stephen F Elston. All rights reserved. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
