{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 13     \n",
    "# Introduction to Hypothesis Testing     \n",
    "\n",
    "\n",
    "## Introduction    \n",
    "\n",
    "Hypothesis testing is a fundamental process in statistical inference. Hypothesis testing seeks to answer the question, are the differences in the distributions of certain variables **statistically significant**? The concept of statistical significance must not be confused with the question, are differences in the distributions of certain variables important to the problem being addressed? While hypothesis testing helps to identify candidates with important differences, the method cannot and does not measure importance.    \n",
    "\n",
    "A concept which is often difficult to grasp is that many hypothesis testing methods focus only on a **null hypothesis**. A null hypothesis is the hypothesis that there is no significant difference between the distributions of the variables being considered. The goal of the test is to **accept a null hypothesis**, or **reject a null hypothesis**. Notice, that this process does not directly consider if an **alternative hypothesis** is true or not. In fact, there may be many alternative hypotheses!  \n",
    "\n",
    "## Significance Cannot Determine Importance\n",
    "\n",
    "When performing inference we often speak of **statistical significance**. However, one must never confuse this concept with actual importance. Statistical significance is a statement concerning probability. But, the importance of a relationship to the problem at hand can be an entirely different matter. Early workers in statistical inference recognized this fact. For example, Ronald A. Fisher [-@Fisher_1932] warned:    \n",
    "\n",
    "\"*I believe that no one who is familiar, either with mathematical advances in other fields, or with the range of special biological conditions to be considered, would ever conceive that everything could be summed up in a single mathematical formula, however complex.*\"    \n",
    "\n",
    "So what use is statistical significance then? Many statisticians would say that a statistically significant result indicates a relationship which is worthy of further investigation. Further investigation can take many forms. A few examples include:   \n",
    "\n",
    "- Gather more data, either observational or experimental. \n",
    "- Find other variables which might illuminate the relationship under investigation.   \n",
    "- Consider the theoretical basis of the relationship. Can known science add understanding?  \n",
    "\n",
    "In summary, do not confuse statistical significance with actual importance. Statistical significance can help identify important relationships, but is not evidence of actual importance.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps of Hypothesis Testing\n",
    "\n",
    "Let's consider a simple and practical scenario for hypothesis testing. \n",
    "\n",
    "1. **Identify a null hypothesis that can be tested:** In this example,we want to know if using a larger logo on a web site results in significantly different use. Use is measured in terms of number of clicks per session. In this case, the **null hypothesis**, denoted $H_0$, is that the size of the logo does not matter. More formally, this null hypothesis is the population assumption of no difference in use. \n",
    "2. The **alternative hypothesis** is usually stated in terms of a **treatment** and denoted $H_a$. In this example the treatment is a web site with a larger logo. And, $H_a$ is a population with a significant difference in the population of web site use.  \n",
    "3. **Choose a test statistics:** The test statistic is used as the criteria to determine the significance of the test result.     \n",
    "4. **Select a criteria to evaluate the hypothesis:** A **significance level** or **cutoff** probability is specified. This cutoff value is the level of **confidence** required that the difference in the computed test statistic **does not occur by randomness alone**. The cutoff should be selected based on how much confidence is required that the population difference is statistically significant. Typically used cutoff values include, 0.1, 0.05, and 0.01. The correct cutoff is problem specific. If our sample has a probability of $<= 10\\%$ that randomness alone caused the observed difference we will **reject the null hypothesis** that the logo does not mater. This criteria is known as the **cutoff**. \n",
    "5. **acquire a random sample from the population:** The population in this cases is all possble users of the web site. Randomly draw a number of users for each **treatment**. In statistical terminology, a treatment is the factor that differentiates the populations being compared with the test. For our example, the treatments are the current web site or the one with a larger logo (e.g. A or B). \n",
    "6. **Calculate the value of the test statistic** to compare the **responses**. If the test statistic is beyond the cutoff value we say the differences are **significant** and reject the null hypothesis. \n",
    "\n",
    "The foregoing seems like a simple recipe, but there are many pitfalls.   \n",
    "- **Rejecting the null hypothesis does not mean we should accept our proposed alternative**. Failing to rejecting the null hypothesis can occur for several reasons, including:   \n",
    "  - The alternative hypothesis was false to begin with.\n",
    "  - We did not collect enough evidence given the **size of the effect**. Roughly speaking, the effect size is difference in populations under the different treatments. We will explore this aspect of the problem in the **power** discussionn below.   \n",
    "  - There can be multiple alternatives. In our example, even in the case we reject the null hypothesis, the trafic on the web site could be greater or less than the baseline site A.  \n",
    "- **Significance simply means the cutoff value has been exceeded, but does not mean the inference is actually important in human terms**. Even if the null hypothesis is rejected, we cannot answer the question of the business impact of the new web site actually being important to the company from the hypothesis test alone. \n",
    "\n",
    "We will have more to say about these and other complications latter. \n",
    "\n",
    "To proceed with the exercises, execute the code in the cell below to import the required packages.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import pandas as pd\n",
    "from statistics import mean\n",
    "import scipy.stats as ss\n",
    "import statsmodels.stats.weightstats as ws\n",
    "import statsmodels.stats.power as smsp\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing and the Cut-Off   \n",
    "\n",
    "The primary idea of a statistical test is determining if the value of the chosen **test statistic** exceeds a cutoff value. We select the cutoff value based on the **confidence** we wish to have in the test result. For example, by specifying a cutoff of $0.05$ we are saying that the probability that the test statistic exceeding the cutoff is $0.05$. Once the cutoff value has been set and the test statistic computed, we interpret the results as follows:         \n",
    "- If the test statistic does not exceed the cutoff value, we accept the null hypothesis.   \n",
    "- On the other hand, if the test statistic exceeds the cutoff we reject the null hypothesis. \n",
    "\n",
    "The foregoing seems straight forward enough. But, there is one other significant consideration. Which tails of the distribution of the test statistic are considered? The choice depends on the nature of the null hypothesis being tested. For a cutoff value, $\\alpha$, are three possible choices:   \n",
    "1. **Two-sided:** In this case, we test the computed value of the test statistic is significantly different than the null hypothesis. The difference can be positive or negative. A cutoff value of $\\alpha/2$ is used on both the upper and lower tail of the test statistic distribution.    \n",
    "2. **One-sided upper tail:** In this case, the null hypothesis is that the test statistic is less than the cutoff value $\\alpha$. The cutoff is applied only to the upper tail of the test statistic distribution. In other words, this case is a test that the computed test statistic is significantly greater than the null value.      \n",
    "3. **One=sided lower tail:** Finally, we have the case where the null hypothesis is that the test statistic is greater than the cutoff value $\\alpha$. The cutoff is applied only to the lower  tail of the test statistic distribution. This case is a test that the computed test statistic is significantly less than the null value.      \n",
    "\n",
    "You can illustrate these cases by executing the code in the cell below and examining the resulting plots.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## figure1 shoing cutoff \n",
    "def cutoff_polygon(upper, cutoff, ax, step=0.02):\n",
    "    ## Plot the cuttoff area \n",
    "    lower = norm.ppf(cutoff)\n",
    "    if(lower > upper):\n",
    "        ax.vlines(lower, 0.0, 0.15, color='red', linewidth=3)\n",
    "        ax.text(lower-1.6, 0.16, 'Cutoff value')\n",
    "        temp = upper\n",
    "        upper = lower\n",
    "        lower = temp  \n",
    "    else:\n",
    "        ax.vlines(lower, 0.0, 0.15, color='red', linewidth=3)\n",
    "        ax.text(lower, 0.16, 'Cutoff value')\n",
    "    x = list(np.arange(lower, upper, step))\n",
    "    norm_vals = list(np.apply_along_axis(lambda x: norm.pdf(x), 0, x))\n",
    "    x = x + [upper, lower]   \n",
    "    norm_vals = norm_vals + [0.0, 0.0]\n",
    "    ax.fill(x, norm_vals, 'black')\n",
    "    \n",
    "def plot_norm(start, end, ax, title, step=0.2):\n",
    "    ## Plot the Normal distribution \n",
    "    x = np.arange(start, end, step=0.02)\n",
    "    norm_value = np.apply_along_axis(lambda x: norm.pdf(x), 0, x)\n",
    "    ax.plot(x, norm_value, linewidth=3, color='black')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(' ')\n",
    "    ax.axhline(0.0, color='black')\n",
    "\n",
    "## Set up the plot grid\n",
    "plt.rc('font', size=18)\n",
    "fig, ax = plt.subplots(2, 2, figsize=(20, 11)) \n",
    "alpha = 0.05\n",
    "\n",
    "## Two sided\n",
    "plot_norm(-4.0, 4.0, ax[0,0], title='Two sided, cutoff = ' + str(alpha))\n",
    "cutoff_polygon(4.0, 1.0- alpha/2.0, ax[0,0])\n",
    "cutoff_polygon(-4.0, alpha/2.0, ax[0,0])\n",
    "ax[0,1].axis('off')\n",
    "\n",
    "## Upper\n",
    "plot_norm(-4.0, 4.0, ax[1,0], title='One sided, upper, cutoff = ' + str(alpha))\n",
    "cutoff_polygon(4.0, 1.0- alpha, ax[1,0])\n",
    "\n",
    "## Lower\n",
    "plot_norm(-4.0, 4.0, ax[1,1], title='Two sided, cutoff = ' + str(alpha))\n",
    "cutoff_polygon(-4.0, alpha, ax[1,1])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is  the p-value?\n",
    "\n",
    "Results of hypothesis tests are often reported as a p-value. You must be careful to correctly interpret the p-value. In summary, we can characterize the p-value as follows:    \n",
    "\n",
    "- **Significance** of the test is determined by the p-value. If the p-values is less than the cutoff, the effect is considered significant. \n",
    "  * **Beware!:** statistical significance is not a measure of actual importance. \n",
    "  * Significance only means that there is a reasonable probability the null hypothesis should be rejected. Further investigation, including applying **domain knowledge**, is required to determine if difference is actually important.  \n",
    "- In technical terms, a p-value is the probability of obtaining an effect **at least as extreme** as the one in your sample data, assuming the null hypothesis is true.\n",
    "For example, for a vaccine study with a p-value of 0.04, you’d obtain the observed difference or more in 4% of studies due to random sampling error.\n",
    "**P values address only one question: how likely are your data, assuming a true null hypothesis?** \n",
    "- **P value does not measure support for the alternative hypothesis!**\n",
    "   * For example, the p-value of 0.04 does not mean there is a 0.96 probability that the effect of the new vaccine is significantly different. \n",
    "   * A smaller p-value does not mean a result is more significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common mistakes with P-Values\n",
    "\n",
    "There are many ways to misinterpret or misuse p-values. Perhaps, the most common errors are these:\n",
    "\n",
    "1. Interpreting a P value as the probability of mistakenly rejecting a true null hypothesis (a **type I error**).\n",
    "  - P values calculations assume the null hypothesis is true for the population and the difference means is due to sampling. **P values can not tell you the probability that the null is true or false!** \n",
    "  - For the vaccine study, correct and incorrect ways to interpret a P value of 0.04 include:\n",
    "    * Correct: If vaccine has no effect,  **the observed difference or more arises solely from random sampling in 4% of studies.**\n",
    "    * Incorrect: By rejecting the null hypothesis, there is a 4% chance of Type 1 error.\n",
    "2. The second common mistake: you can deduce the probability that the alternative hypothesis is correct (e.g. $1.0 - p$, or a **type II error**).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Tests for Means - the t-test    \n",
    "\n",
    "As you might expect from the name, the t-test is based on tests on a t-distribution. It turns out that the difference in means between two Normal distributions follows a t-distribution. The t-distribution has many nice properties including:\n",
    "\n",
    "- The t-distribution is the natural distribution for tests on differences of means. \n",
    "- The t-distribution has heavier tails than the Normal and relaxes assumptions on the differences.\n",
    "- For high degrees of freedom, the t-distribution converges to a Normal distribution, by the Central Limit Theorem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Who was Student?\n",
    "\n",
    "The basic form of the test is referred to as the Student t-test. Student was a pseudonym for William Sealy Gosset. Gosset worked for Guinness Company and perform experiment to improve brewing. \n",
    "\n",
    "Gosset published the theory of the t-test in 1908 under his pseudonym, 'Student'. There is an apocryphal story that Gosset used a pseudonym since the Guinness company forbid employees from publishing research. However, recently this story has been disproven, as the Guinness company never had such a rule.    \n",
    "\n",
    "<img src=\"../images/Gosset_1908.jpg\" alt=\"Drawing\" style=\"width:300px; height:400px\"/>\n",
    "<center>William Sealy Gosset in 1908</center>\n",
    "\n",
    "<img src=\"../images/gossett.jpg\" alt=\"Drawing\" style=\"width:500px; height:250px\"/>\n",
    "<center>Plaque at site of Gosset's home</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why the t-distribution\n",
    "\n",
    "As you might expect from the name, the t-test is based on tests on a t-distribution. It turns out that the difference in means between two Normal distributions follows a t-distribution. The t-distribution has many nice properties including:\n",
    "\n",
    "- The t-distribution is the natural distribution for tests on differences of means. \n",
    "- The t-distribution has heavier tails than the Normal and relaxes assumptions on the differences.\n",
    "- For high degrees of freedom, the t-distribution converges to a Normal distribution, by the Central Limit Theorem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  A first t-test example\n",
    "\n",
    "As a first example, compare the means of two Normal distributions. Execute the code in the cell below to compute samples from two Normal distributions with slightly different means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=233423)\n",
    "pop_A = norm.rvs(loc=150, scale=7, size=25)\n",
    "pop_B = norm.rvs(loc=153, scale=4, size=35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below creates histograms of the two populations along with a line for the mean of each population. Execute this code and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_plot(vec, bins, ax):\n",
    "    plt.rc('font', size=12)\n",
    "    ax.hist(vec, bins = bins)\n",
    "    ax.axvline(mean(vec), color = 'red')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_xlabel('Value')\n",
    "    \n",
    "def plot_pop(a, b, cols=['pop_A', 'pop_B'], nbins = 20):\n",
    "    fig, ax = plt.subplots(2,1, figsize=(8,8))\n",
    "    minx = min([min(a), min(b)])\n",
    "    maxx = max([max(a), max(b)])\n",
    "    stepx = (maxx - minx)/(nbins + 1)\n",
    "    bins = [minx + i * stepx for i in range(nbins + 1)]\n",
    "    hist_plot(a, bins, ax[0])\n",
    "    ax[0].set_title('Histograme of ' + cols[0] + ' and ' + cols[1])\n",
    "    hist_plot(b, bins, ax[1])\n",
    "\n",
    "plot_pop(pop_A, pop_B)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now compute the two-sided t-test to determine if the difference of means is significant. A number of summary statistics are computed and printed for the test. The two-sided t-test is used to determine if we can reject the null hypothesis that the difference of means is not significant. \n",
    "\n",
    "In this case, we know we cannot assume equal variance of the two populations. Therefore, we use Welch's t-test. \n",
    "\n",
    "Computing a t-test including the usual summary statistics with [statsmodels.stats](https://www.statsmodels.org/stable/stats.html), requires several steps. You can the t-statistic and see the results by executing the code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_test(a, b, alpha, alternative='two-sided', usevar = 'unequal'):\n",
    "    '''Function to compute a two sample t-test on means'''\n",
    "    \n",
    "    ## Compute the difference in means for reporting.\n",
    "    diff = a.mean() - b.mean()\n",
    "\n",
    "    ## Compute the t-test\n",
    "    t, p, df = ws.ttest_ind(a, b, usevar=usevar)\n",
    "    \n",
    "    ## Find the confidence interval\n",
    "    means = ws.CompareMeans(ws.DescrStatsW(a), ws.DescrStatsW(b))\n",
    "    confint = means.tconfint_diff(alpha=alpha, alternative=alternative, usevar=usevar) \n",
    "\n",
    "    index = ['DegFreedom', 'Difference', 'Statistic', 'PValue', 'Low95CI', 'High95CI']\n",
    "    return pd.Series([df, diff, t, p, confint[0], confint[1]], index = index)  \n",
    "   \n",
    "\n",
    "test = t_test(pop_A, pop_B, 0.05)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine these statistics noticing the following:\n",
    "1. The **degrees of freedom** determine the shape of the t-distribution used for the test.\n",
    "2. **Difference in means** is small.\n",
    "3. **t-statistic is small and the p-value is large**. There is a high chance that the difference in means is **not significant**. \n",
    "4. The **95% confidence interval straddles 0**, indicating that a 0 difference in means is within the CI.  \n",
    "\n",
    "Based on the above statistics we **cannot reject the null hypothesis**. As we have already said, this does not mean we accept the null hypothesis. \n",
    "\n",
    "The code in the cell below makes a similar plot to the ones you have already created, but with the upper and lower confidence bounds shown as dashed lines. Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_mean_ci(vec, t_test, bins, ax):\n",
    "    upper = mean(vec) + t_test[5] - t_test[1]\n",
    "    lower = mean(vec) + t_test[4] - t_test[1]\n",
    "    ax.hist(vec, bins = bins)\n",
    "    ax.axvline(mean(vec), color = 'red')\n",
    "    ax.axvline(upper, color = 'red', linestyle='--')\n",
    "    ax.axvline(lower, color = 'red', linestyle='--')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_xlabel('Value')\n",
    "\n",
    "def plot_t(a, b, t_test, cols=['pop_A', 'pop_B'], nbins = 20):\n",
    "    fig, ax = plt.subplots(2,1, figsize=(8,8))\n",
    "    minx = min([min(a), min(b)])\n",
    "    maxx = max([max(a), max(b)])\n",
    "    stepx = (maxx - minx)/(nbins + 1)\n",
    "    bins = [minx + i * stepx for i in range(nbins + 1)]\n",
    "    hist_mean_ci(a, t_test, bins, ax[0])\n",
    "    ax[0].set_title('Histograme of ' + cols[0] + ' and ' + cols[1])\n",
    "    hist_mean_ci(b, t_test, bins, ax[1])\n",
    "    \n",
    "plot_t(pop_A, pop_B, test)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the mean of the population shown in the lower plot is within the confidence interval of the mean of the population in the upper plot and vice versa. But, the question remains, is the difference of means of these two populations significant? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 13-1:** In the previous example the difference of means was not significant. Now, you will repeat this analysis but with a slightly greater difference in means. Create two populations with means (`loc`) of `150` and `156`, using the same scale and a random seed of 357. Determine if this difference in means is significant using the `t_test` and `plot_t` functions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr.seed(357)\n",
    "## Put your code below\n",
    "  \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Using a cut-off value of 0.05, based on the results of your test, is the difference of means of these distributions significant and why?    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power of Tests\n",
    "\n",
    "The **power of a test** is formally defined as:\n",
    "\n",
    "$$power = P(reject\\ H_0| when\\ H_a\\ is\\ true)$$\n",
    "\n",
    "In pain language, the **power is the probability of getting a positive result when the null hypothesis is not true**. Conversely, a test with **insufficient power will not detect a real effect**. Clearly, we want the most powerful test we can find for the situation. A powerful test gives a better chance of detecting an effect. \n",
    "\n",
    "Computing test power can be a bit complex, and analytical solutions can be difficult or impossible. Often, a simulation is used to compute power. \n",
    "\n",
    "Let's look at the example of computing power for the two sample t-test for the difference of means. The power of this test depends on the several parameters:   \n",
    "\n",
    "- The type of test.  \n",
    "- The number of samples.   \n",
    "- The anticipated difference in the population means, which we call the **effect size**.   \n",
    "- The significance level of  the test.   \n",
    "\n",
    "When running a power test, you can ask several questions, which will assist you in designing an experiment:   \n",
    "- Most typically, you will determine how big a sample is required to have good chance of rejecting the null hypothesis given the expected effect size. \n",
    "  * Traditionally, a power level of 0.8 is considered adequate. However, at this power level there is a 1 in 5 chance of not detecting the desired effect.  \n",
    "  * One should carefully consider the power level required for the situation at hand.\n",
    "- Or, you can also determine how big an effect needs to be given a fixed sample size (all the samples you have or can afford) to have a good chance of rejecting the null hypothesis.  \n",
    "- Finally, you can determine the how different cutoff values change the likelihood of finding an effect.   \n",
    "\n",
    "The Python statsmodels package provides power calculations for a limited set of hypothesis tests. We can use these capabilities to examine the power of t-tests under various assumptions. \n",
    "\n",
    "The code in the cell below does the following:\n",
    "\n",
    "- Create a sequence of effect sizes.\n",
    "- Compute a vector of power values for the effect size using [statsmodels.stats.power.tt_ind_solve_power](https://www.statsmodels.org/stable/generated/statsmodels.stats.power.tt_ind_solve_power.html).\n",
    "- Plot the effect size vs. power. \n",
    "\n",
    "Execute this code and examine the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr.seed(seed=23344)\n",
    "diffs = np.arange(start = 0.0, stop = 1.6, step = .015)\n",
    "powers = [smsp.tt_ind_solve_power(effect_size = x, nobs1 = 25, \n",
    "           alpha = 0.05, power = None, ratio = 1.0, alternative = 'two-sided') \n",
    "          for x in diffs]\n",
    "\n",
    "def plot_power(x, y, xlabel, title):\n",
    "    fig,ax = plt.subplots(figsize=(8,6))\n",
    "    ax.plot(x, y, color = 'red', linewidth = 2)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel('Power')\n",
    "plot_power(diffs, powers, xlabel = 'Difference of means', title = 'Power vs. difference of means')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine these results and notice how the power of the t-test rapidly increases as the difference in means increases. At a relatively small difference in means the power of the test is approaching 1.0, the maximum possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 13-2:** One can also determine the effect of changing the cut-off value, $\\alpha$, on the power of a test. In the code cell below, compute and plot 100 values of test power for significance levels in the range $\\{0.001, 0.25\\}$. Set `n = 25`, `d = 1.0`, for the number of samples and the difference in means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For a cut-off value of 0.05 is the probability of a Type 2 error less that 0.9 and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 13-3:** The third factor one can examine when examining test power is the sample size. In the cell below, compute and plot 100 values of test power for sample size in the range $\\{1, 100\\}$. Set `d = 1.0`, and `sig.level = 0.05`, for the difference in means and the significance level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below\n",
    " \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What is the approximate minimum sample size required to achieve a Type 2 error probability of less than 0.05 and why?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copyright 2018, 2019, 2020, 2021, 2022 Stephen F Elston. All rights reserved.   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
