{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 19    \n",
    "# Linear models; the workhorse of statistics\n",
    " \n",
    "\n",
    "## Introduction\n",
    "\n",
    "The concept of the linear model is the basis of many statistical and machine learning models. Further, an understanding of linear models is a good basis for understanding many other types of statistical and machine learning models.   \n",
    "\n",
    "In this chapter we will focus on regression models, but the lessons drawn from this discussion can be applied to many other types of models. By developing an understanding of linear regression, you are building a foundation to understand many other machine learning models. Nearly all machine learning methods suffer from the same problems, including over-fitting and mathematically unstable fitting methods. Understanding these problems in the linear regression context will help you work with other machine learning models.     \n",
    "\n",
    "The method of regression is one of the oldest and most widely used analytics methods. The goal of regression is to produce a model that represents the **best fit** to some observed data. Typically the model is a function describing some type of curve (lines, parabolas, etc.) that is determined by a set of parameters (e.g., slope and intercept). *Best fit* means that there is an optimal set of parameters which minimize an error criteria we choose.     \n",
    "\n",
    "Many machine learning models, including some of the latest deep learning methods, are a form of regression. **Linear regression** is the foundational form of regression. Linear regression minimizes squared error of the predictions of the dependent variable using the values of the independent variables. This approach is know as the **method of least squares**.   \n",
    "\n",
    "Regression models attempt to predict the value of one **dependent variable** using the information from other **independent variables**. Unfortunately, the terminology used for these variables is not consistent across authors, statistical software packages, and application domains. The table below list some, but by no means all of the terms used for these variables. \n",
    "\n",
    "### Confusing terminology \n",
    "\n",
    "Given that linear models have been developed in many areas for a long period of times, different terminology has developed for the same things. For people trying to learn the subject this differing terminology is confusing and seemingly conflicting.    \n",
    "\n",
    "The main division in terminology arises from different communities within statistics and machine learning. The table below shows some of the different terms commonly used in the two lineages:       \n",
    "\n",
    "| Machine Learning Terminology | Statistical Terminology          |\n",
    "|:---------------------------|:------------------------------|\n",
    "| Regression vs classification   | Continuous numeric vs categorical response      |\n",
    "| Learning algorithm or model    | Model                                |\n",
    "| Features                       | Predictor, explanatory, exogenous, or independent variables   |\n",
    "| Training                       | Fitting                              |\n",
    "| Trained model                  | Fitted model                         |\n",
    "| Supervised learning            | Predictive modeling      \n",
    "\n",
    "For the specific case of regression there are further differences in terminology. These arise not just between the statistical and machine learning communities. One difference in terminology is the naming of the variables used in regression and other machine learning models. The table below outlines some of these differences:          \n",
    "\n",
    "Predicted Variable | Variables Used to Predict    \n",
    ":----------------------- | :------------------------------     \n",
    " y | x   \n",
    " Dependent | Independent    \n",
    " Endogenous | Exogenous    \n",
    " Response | Predictor    \n",
    " Response | Explanatory    \n",
    " Label | Feature    \n",
    " Regressand | Regressors    \n",
    " Outcome | Design   \n",
    " Left Hand Side | Right Hand Side     \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## History\n",
    "\n",
    "Regression is based on the method of least squares or the method of minimum mean square error. The idea of averaging errors have been applied for nearly three centuries. The first known publication of a *method of averages* was by the German astronomer Tobias Mayer in 1750. Lapace used a similar method which he published in 1788.\n",
    "\n",
    "<img src=\"../images/TobiasMayer.jpg\" alt=\"TobiasMayer\" style=\"width: 200px;\"/>\n",
    "<center>Credit wikipedia commons</center>\n",
    "\n",
    "The first publication of the **method or least squares** was by the French mathematician Adrien-Marie Legendre in 1805. Legendre was a brilliant mathematician, known for his unpleasant personality.  \n",
    "\n",
    "![](../images/Legendre.jpg)\n",
    "<center>Caricature of Legendre, published method of least squares: credit Wikipedia commons</center>\n",
    "\n",
    "It is very likely that the German physicist and mathematician Gauss developed the method of least squares as early as 1795, but did not publish the method until 1809, aside from a reference in a letter in 1799. Gauss never disputed Legendre's priority in publication. Legendre did not return the favor, and opposed any notion that Gauss had used the method earlier. \n",
    "\n",
    "![](../images/Carl_Friedrich_Gauss.jpg)\n",
    "<center>Carl Friedrich Gauss, early adopter of the least squares method: credit Wikipedia commons</center>\n",
    "\n",
    "The first use of the term **regression** was by Francis Gaulton, a cousin of Charles Darwin, in 1886. Gaulton was interested in determining which traits of plants and animals, including humans, could be said to be inherited. Gaulton used the term **regression to the mean** to describe the natural processes he observed in inherited traits.  \n",
    "\n",
    "<img src=\"../images/Francis_Galton.jpg\" alt=\"Drawing\" style=\"width:225px; height:250px\"/>\n",
    "<center>Francis Galton, inventor of regression: credit Wikipedia commons</center>\n",
    "\n",
    "While Gaulton invented a form regression, it fell to Karl Pearson to put regression and multiple regression on a firm mathematical footing. Pearson's 1898 publication proposed a method of regression as we understand it today. \n",
    "\n",
    "Many others have expanded the theory of regression in the 120 years since Pearson's paper. Notably, Joseph Berkson published the logistic regression method in 1944, one of the first classification algorithms. In recent times, the interest in machine learning has lead to a rapid increase in the variety of regression models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Theory of Linear Regression\n",
    "\n",
    "We will focus on the theory of **linear models**, which are foundational. Key properties of linear models include:\n",
    "- Derived with linear algebra.\n",
    "- Include any model **linear in coefficients**, including polynomials, splines, Gaussian kernels and many other nonlinear functions.    \n",
    "- Understanding linear models is basis for understanding behavior of many other statistical or machine learning models.\n",
    "- Linear models are the basis of many time series and survival models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear model of a strait line\n",
    "\n",
    "Let's have a look at the simple case of a regression model for a straight line. For this example we will work with single regression with one feature and one label. The data are in the form of some number of values pairs, $\\{x_i,y_i \\}$. \n",
    "\n",
    "The goal of this regression model is to find a straight line that best fits the observed data. We can define the line by two coefficients or **parameters**, the **slope** and the **intercept**. A general representation of this parameterization of a straight line is illustrated in the figure below.\n",
    "\n",
    "<img src=\"../images/ymxb.jpg\" alt=\"y_equals_mx_plus_b\" style=\"width: 450px;\"/>\n",
    "<center>**Single regression model: credit wikipedia commons**</center>\n",
    "\n",
    "Where,  \n",
    "\n",
    "\\begin{align}\n",
    "m &= slope = \\frac{rise}{run} = \\frac{\\delta y}{\\delta x}\\\\\n",
    "and\\\\\n",
    "y &= b\\ at\\ x = 0\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "For each of the pairs of observed values, ${x_i,y_i}$, we can write the equation for the line with the errors as:\n",
    "\n",
    "\\begin{align}\n",
    "y_i &= mx_i + b + \\epsilon_i \\\\\n",
    "where \\\\\n",
    "\\epsilon_i &= error\n",
    "\\end{align}\n",
    "\n",
    "We can visualize these errors as shown in the figure below.\n",
    "\n",
    "<img src=\"../images/LSRegression.jpg\" alt=\"LSRegression\" style=\"width: 450px;\"/>\n",
    "<center>Example of least squares regression with errors shown as vertical lines: credit wikipedia commons</center>\n",
    "\n",
    "Notice that these errors are only along the y-axis. In other words, the **linear regression model accounts for errors in the predicted value**, not errors in the values of the independent values. We want to solve for $m$ and $b$ by minimizing these errors, $\\sum_i \\epsilon_i$. This leads us to the  **least squares regression** problem.\n",
    "\n",
    "$$min \\Sigma_i \\epsilon^2 = min \\Sigma_i{ (y_i - (mx_i + b))^2}$$\n",
    "\n",
    "One can see the equivalence to the Normal log-likelihood:   \n",
    "\n",
    "$$l(\\mathbf{X}\\ |\\ \\mu, \\sigma ) = - \\frac{n}{2} log( 2 \\pi \\sigma^2 ) - \\frac{1}{2 \\sigma^2} \\sum_{j=1}^n (x_j - \\mu)^2$$\n",
    "\n",
    "For a fixed variance, $\\sigma^2$, the log-likelihood is maximized when the least square error condition in met. This observation has two important implications:   \n",
    "1. The error, or **residuals**, arising from a least squares model are expected to be Normally distributed.   \n",
    "2. There are computationally efficient algorithms for finding minimums of equations. In a previous chapter we investigated the stochastic gradient descent (SGD) algorithm for maximum likelihood estimation (MLE). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression assumptions\n",
    "\n",
    "Before going any further, we should discuss a few key assumptions of linear regression, also known as **ordianary least squares (OLS)**. Keep these points in mind whenever you use a regression model. \n",
    "\n",
    "1. There is a **linear relationship** between dependent variable and the **coefficients** of the independent variables. This does not mean the function approximation used must be linear. Only that the model must be linear in the coefficients. \n",
    "2. Measurement error is independent and random. Technically, we say that the error is **independent identical distributed, or iid**.\n",
    "3. Errors arise from the dependent variable only. Other models, such as complete regression, must be used if there are errors in the independent variable. \n",
    "The diagram below illustrates the iid errors for the dependent variable only.\n",
    "\n",
    "![](../images/IndependentErrors.jpg)\n",
    "<center>Credit wikipedia commons</center>\n",
    "\n",
    "4. There is no **multicolinearity** between the features or independent variables. In other words, there is no significant correlation between the features.\n",
    "5. The **residuals** are independent identically distributed (iid) Normal and **homoscedastic** (constant variance).  In other words, the errors are the same across all values of the independent variables. We will explore this fundamental property further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A First Regression Model\n",
    "\n",
    "Let's give regression a try. The code in the cell below computes data pairs along a straight line. Normally distributed noise is added to the data values. Run this code and examine the head of the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics.regressionplots import influence_plot, plot_regress_exog\n",
    "from sklearn.preprocessing import normalize, StandardScaler\n",
    "import scipy.stats as ss\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import patsy\n",
    "%matplotlib inline\n",
    "\n",
    "# Paramters of generated data\n",
    "n_points = 50\n",
    "x_start, x_end = 0, 10\n",
    "y_start, y_end = 0, 10\n",
    "y_sd = 1\n",
    "\n",
    "# Generate data columns\n",
    "nr.seed(5666)\n",
    "x_data = np.linspace(x_start, x_end, n_points) # The x values\n",
    "y_error = np.random.normal(loc=0, scale=y_sd, size=n_points) # The Normally distributed noise\n",
    "y_data = np.linspace(y_start, y_end, n_points) + y_error + 1.0 # The y values including an intercept\n",
    "\n",
    "# Put data in dataframe\n",
    "sim_data = pd.DataFrame({'x':x_data, 'y':y_data})\n",
    "\n",
    "sim_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you can visualize these data by executing the code in the cell below. Notice that the points nearly fall on a straight line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib may give some font errors when loading for the first time, you can ignore these\n",
    "fig,ax = plt.subplots(figsize=(7,7))\n",
    "ax.plot(sim_data['x'], sim_data['y'], 'ko')\n",
    "ax.grid(True)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title('x vs y')\n",
    "ax.set_ylim(0,11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centering explanatory variable\n",
    "\n",
    "We want to be in a position to interpret the results of the linear model. The problem is the intercept term. By definition, the intercept is the crossing point of the regression line on the y-axis (dependent) variable. The independent variables must all have values of 0, at the intercept. However, this intercept value is often quite arbitrary and meaningless in terms of the values of the independent (predictor) variables. In fact, the independent variables may not reasonably ever have values of 0.  As an example, the intercept can have impossible values, such as a negative life expectancy.     \n",
    "\n",
    "The solution is to **center** the independent variables. Centering these variables transforms the intercept term to the **mean of the response variable**. This practice eliminates the aforementioned problems by ensuring the intercept point is in a reasonable range of the independent variables.       \n",
    "\n",
    "> ***Note:** It is standard practice to transform independent variables to be both zero mean (centered) and unit variance. This treatment can be essential if the scale of multiple independent variables is quite different. If variance standardization is not applied in such cases, variables with a large range of numeric values can dominate the model training. Rather, we want variables with the best explanatory power to dominate model training regardless of the range of values. For single (independent variable) regression, scalling is not necessary. See the section below of scaling data for more detail. \n",
    "\n",
    "Execute the code in the cell below to compute a centered independent variable.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_data['x_centered'] = np.subtract(sim_data.loc[:,'x'], np.mean(sim_data.loc[:,'x']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Fitting a Linear Regression Model\n",
    "\n",
    "Now, you are ready to build and evaluate a regression model using Python. There are a number of Python libraries that contain linear modeling capabilities.\n",
    "\n",
    "The [scikit-learn](https://scikit-learn.org/stable/) package has many different types of machine learning algorithms. Scikit-lean model interfaces take a machine learning perspective. The data arguments for Scikit-learn models are numpy arrays which must be dimensioned properly.\n",
    "\n",
    "[Statsmodels](https://www.statsmodels.org/stable/index.html) is another Python package with extensive linear model capability. This package takes a statistical perspective, which we adopt here. A nice feature of statsmodels is that the data argument is a Pandas data frame. \n",
    "\n",
    "You can specify statsmodels models using the [R-style model language](https://www.statsmodels.org/devel/example_formulas.html). If you are not familiar with the R model language interface, read the summary below before proceeding. For those that have experience with the R programming language, statsmodels will seem familiar since it provides a R-like model language interface. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **R-Style Model Formulas**    \n",
    "> The code in the cell below uses an R style model formula. This modeling language was introduced in [Chambers and Hastie, 1992, Statistical Models in S](https://www.taylorfrancis.com/books/e/9780203738535).     \n",
    ">\n",
    "> In statsmodels there is a specific implementation of the R formula language, documented [here](https://www.statsmodels.org/stable/examples/notebooks/generated/formulas.html). For a good [**cheatsheet and summary of the R modeling language**](http://faculty.chicagobooth.edu/richard.hahn/teaching/formulanotation.pdf) look at the posting by Richard Hahn of the Chicago Booth School.    \n",
    ">\n",
    "> Models are defined by an equation using the $\\sim$ symbol, meaning *modeled by*. The variable to be modeled is always on the left. The relationship between the independent variables shown on the right. This basic scheme can be written: \n",
    "$$dependent\\ variable\\sim indepenent\\ variables$$\n",
    "> For example, if the dependent variable (dv) is modeled by two independent variables (var1 and var2), with no interaction, the formula is:\n",
    "$$dv \\sim var1 + var2$$\n",
    "> - Example; dependent variable (dv) is modeled by independent variables (var1) and its square. The $I()$ operator is used to wrap a function of a variable:\n",
    "$$dv \\sim var1 + I(var1**2)$$\n",
    "> - Example; dependent variable (dv) is modeled by two independent variables (var1 and var2) and the **interaction term** between them:\n",
    "$$dv \\sim  var1*var2$$\n",
    "> The expansion of this notation is: \n",
    "\\begin{align}\n",
    "var1:var &= 1 + var1 + var 2 + var1:var2\\\\\n",
    "var1*var &= intercept + sum\\ of\\ variables + interaction\\ of\\ variables\n",
    "\\end{align}\n",
    "> - Example; dependent variable (dv) modeled by independent numeric variable (var1) and a categorical variable (var2) using the $C()$ operator to encode the levels of the categorical variable:\n",
    "$$dv \\sim var1 + C(var2)$$\n",
    "> - Example of using $-$ operation to drop terms in from the model. In this case both intercept is dropped by the $-1$ term and the independent $var2$ term is dropped:    \n",
    "\\begin{align}\n",
    "dv &\\sim  -1 - var2 + var1*var2\\\\\n",
    "dv &\\sim no\\ intercept + var1 + interaction\\ of\\ variables\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, there is only one independent variable and one dependent variable. The code in the cell below does the following:  \n",
    "\n",
    "- The model formula is specified as $y \\sim x$.\n",
    "- An [statsmodels.formula.api.ols (ordinary least squares)](https://www.statsmodels.org/stable/generated/statsmodels.formula.api.ols.html?highlight=statsmodels%20formula%20ols#statsmodels.formula.api.ols) model object is specified using the model formula and the data frame. Here, we use the lower case *ols* function so that the model language can be specified in the call. \n",
    "- The *fit* method is applied to the ols object. \n",
    "- The slope and intercept point estimates are printed. \n",
    "\n",
    "Execute this code and note the coefficient values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the regresson model and fit it to the data\n",
    "ols_model = smf.ols(formula = 'y ~ x_centered', data=sim_data).fit()\n",
    "\n",
    "## Print the model coefficient\n",
    "print('Intercept = {0:4.3f}  Slope = {1:4.3f}'.format(ols_model._results.params[0], ols_model._results.params[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intercept and slope are close to the actual values of 1.0 and 1.0. However, we need a more thorough examination of the results before we can say this is a good model for these data.  \n",
    "\n",
    "As a first step toward evaluating this model, we can compute the predicted values of y given the values of x. Execute the code in the cell below which uses the *predict* method to compute these predicted values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predicted to pandas dataframe\n",
    "sim_data['predicted'] = ols_model.predict(sim_data.x_centered)\n",
    "# View head of data frame\n",
    "sim_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a single regression model, we can plot the values of the predicted line along with the actual data values on a 2-dimensional plot. For models with multiple features, [partial regression plots](https://en.wikipedia.org/wiki/Partial_regression_plot) can be created. \n",
    "\n",
    "Execute the code in the cell below to create the plot and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(8,8))\n",
    "ax = sns.lineplot(x='x_centered', y='predicted', data=sim_data, color='red')\n",
    "sns.scatterplot(x='x_centered', y='y', data=sim_data, ax=ax)\n",
    "ax.set_title('Observed and predicted values vs. centered x indpendent variable')\n",
    "_=ax.set_ylim(0,11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, so good. The predicted regression line does seem to fit the data well. But, how can we quantify the performance of this model? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the Model Parameters    \n",
    "\n",
    "How can we interpret the model parameters? First, recall that the model is linear and constructed using a zero-centered independent variable. The result is both simple and intuitive:    \n",
    "- The intercept of about 6.0 is the **mean** of the response variable y.    \n",
    "- The slope coefficient of 0.94 indicates that the response variable increases by 0.94 for each unit of increase of the independent variable.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of regression models\n",
    "\n",
    "Now that you have built a regression model, let's look at how you can quantitatively evaluate the performance of a regression model. There is no one metric that can be used to evaluate a linear model, or any other type of machine learning model. Here as in any other case, we will in fact use multiple metrics to evaluate the linear regression model. \n",
    "\n",
    "The evaluation of regression models is based on measurements of the errors. The errors of a regression model can be visualized as shown in the figure below. \n",
    "\n",
    "<img src=\"../images/Errors.jpg\" alt=\"Regression_Errors\" style=\"width: 450px;\"/>\n",
    "<center>Measuring errors for a regression model: credit, Wikipedia commons</center>  \n",
    "    \n",
    "    \n",
    "Let's start with the observed values of the feature, $X$, and label, $Y$.      \n",
    "\n",
    "\\begin{align}\n",
    "X &= [x_1, x_2, \\ldots, x_n]\\\\\n",
    "Y &= [y_1, y_2, \\ldots, y_n]\\\\\n",
    "where\\\\\n",
    "x_i &= ith\\ feature\\ value\\\\\n",
    "y_i &= ith\\ label\\ value\\\\\n",
    "\\end{align}\n",
    "\n",
    "The results of the regression model are **estimates** which we write:   \n",
    "\n",
    "\\begin{align}\n",
    "\\bar{Y} &= mean(Y)\\\\\n",
    "\\hat{y_i} &= regression\\ estimate\\ of\\ y_i\n",
    "\\end{align}  \n",
    "\n",
    "Given the above we can define the follow **sum of squares** relationships:   \n",
    "\n",
    "\\begin{align}\n",
    "SSE &= sum\\ square\\ explained\\ = \\Sigma_i{(\\hat{y_i} - \\bar{Y})^2}\\\\\n",
    "SSR &= sum\\ square\\ residual\\ = \\Sigma_i{(y_i - \\hat{y_i})^2}\\\\\n",
    "SST &= sum\\ square\\ total\\ = \\Sigma_i(y_i - \\bar{Y})^2 \\\\\n",
    "SST &= SSR + SSE\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $R^2$: variance explained\n",
    "\n",
    "The goal of regression is to minimize the residual error, $SSR$. In other words, when fitting the model we wish to explain the maximum amount of the variance in the original data. We can quantify the **faction of squared error explained** with the **coefficient of determination** also known as $R^2$. We can express $R^2$ as follows:\n",
    "\n",
    "$$R^2 = 1 - \\frac{SSR}{SST}$$\n",
    "\n",
    "The $R^2$ for a perfect model would behave as follows:   \n",
    "\n",
    "\\begin{align}\n",
    "SSR &\\rightarrow 0\\\\\n",
    "which\\ leads\\ to \\\\\n",
    "R^2 &\\rightarrow 1\n",
    "\\end{align}\n",
    "\n",
    "In words, a model which perfectly explains the data has $R^2 = 1$. For a model which does not explain the data at all we can write: \n",
    "\n",
    "\\begin{align}\n",
    "SSR &= SST \\\\ \n",
    "and \\\\ \n",
    "R^2 &= 0\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there are two problems with $R^2$. </center>\n",
    " - $R^2$ is not bias adjusted for degrees of freedom.\n",
    " - More importantly, there is no adjustment for the number of model parameters. As the number of model parameters increases $SSR$ will generally decrease. Without an adjustment you will get a false sense of model performance.    \n",
    " \n",
    "To addresses these related issues, we use **adjusted $R^2$**.\n",
    "\n",
    "\\begin{align}\n",
    "R^2_{adj} &= 1 - \\frac{\\frac{SSR}{df_{SSR}}}{\\frac{SST}{df_{SST}}} = 1 - \\frac{var_{residual}}{var_{total}}\\\\\n",
    "where\\\\\n",
    "df_{SSR} &= SSR\\ degrees\\ of\\ freedom\\\\ \n",
    "df_{SST} &= SST\\ degrees\\ of\\ freedom\n",
    "\\end{align}\n",
    "\n",
    "This gives $R^2_{adj}$ as:\n",
    "\n",
    "\\begin{align}\n",
    "R^2_{adj} &= 1 - (1 - R^2) \\frac{n - 1}{n - k}\\\\ \n",
    "where\\\\\n",
    "n &= number\\ of\\ data\\ samples\\\\\n",
    "k &= number\\ of\\ model\\ coefficients\n",
    "\\end{align}\n",
    "\n",
    "Or, we can rewrite $R^2_{adj}$ as:\n",
    "\n",
    "$$R^2_{adj} =  1.0 - \\frac{SSR}{SST}  \\frac{n - 1}{n - 1 - k}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F test on variance explained        \n",
    "\n",
    "Adjusted $R^2$ is one approach to comparing the variance reduction of a regression model. Another approach is to perform an hypothesis test. For this, we can use the F-test, a test on the ratio of variances. For a model with $k$ model parameters, fitted with $n$ observations. We can compute the F statistic as:     \n",
    "\n",
    "$$F = \\frac{Var_{between}}{Var_{within}} = \\frac{SS_{between}/bdof}{SS_{within}/wdof} \\\\\n",
    "where\\\\   \n",
    "bdof = k-1 \\\\\n",
    "wdof = n - k\n",
    "$$      \n",
    "\n",
    "The F-test on the significance of the F statistic determines the significance of the regression model. A large value of the F-statistic indicates a low probability that the reduction in variance is from random sampling alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of model summary\n",
    "\n",
    "You can see an extensive summary of the fit of the linear model with the *summary* method. Execute the code in the cell below and examine the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ols_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary table gives us a lot of information about the fit of this model. Let's examine some of these values.   \n",
    "\n",
    "- Starting with the **model coefficients**, the report includes an hypothesis test on the statistical significance of the model coefficients. In this case we can interpret this significance as follows: For both the **intercept** and **slope** the t-statistic is large, p-value is small, and the confidence interval does not include zero. These coefficients are statistically significant.\n",
    "- The **F-statistic** and **Prob (F-statistic)** are a measure of the significance of the model against a **null model that does not explain the data**. In this case the large F-statistic and small probability indicate that we can reject this null hypothesis and say the model is significant in terms of explaining the data.  \n",
    "- A related statistic is the [**Omnibus**](https://en.wikipedia.org/wiki/Omnibus_test), which is a test on the likelihood ratio between the model and a null model. Or more specifically, the difference between the log likelihood of the model and the log likelihood of a null model. The null model having no explanitory power. The probability (p-value) in this case allows us to reject the null hypothesis that the model is the same as the null model.   \n",
    "- The $R^2$ value is shown in the upper right corner. The value of 0.91 indicates a relatively good model fit.   \n",
    "- The **adjusted $R^2$** shown in the summary indicates that the model is a good fit. To find this quantity, notice the following: \n",
    "  - The **number of observations** and is the $df_{SST}$. \n",
    "  - The **degrees of freedom residuals** is the $df_{SSR}$. \n",
    "  - Notice that $df_{SST} - df_{SSR} =$ number of model coefficients. \n",
    "- The [**Jarque-Bera**](https://en.wikipedia.org/wiki/Jarque%E2%80%93Bera_test) statistic is a test on the skewness and krutosis of the residuals. Given the large probability (p-value) we cannot reject the null hypothesis that the residuals have significant skewness and krutosis. \n",
    "- The **condition number** is a measure of how well defined the solution is to the system of linear equations solved. A low conditon number ($C \\lt 100$) is generally considered ideal.  \n",
    "  \n",
    "> **Warning:** The hypothesis tests on model coefficients suffer from the same problems of any hypothesis test. These problems are especially prevalent when the are large numbers of model parameters. For example, finding coefficients significant that are not, or vice versa is not uncommon. This situation can be aggravated when features have significant colinearity (correlation). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Residuals\n",
    "\n",
    "There is one more important topic in evaluating regression models, the analysis of the **residuals**. The residuals of a regression model are the difference between the predicted values and actual values of the label. In other words, the residuals are the error term we write as $\\epsilon_i$ for the ith observation.\n",
    "\n",
    "A good linear regression model should have residuals with the following properties: \n",
    "\n",
    "1. The residuals should be approximately **Normally distributed with zero mean**. This criteria applied to any regression model using a least squares loss function. The least squares fitting criteria is only optimal for Normally distributed and zero mean residuals. We can express this important relationship mathematically as:  \n",
    "\n",
    "\\begin{align}\n",
    "y_i &=  mx_i + b + \\epsilon_i \\\\\n",
    "where, \\\\\n",
    "\\epsilon_i &= N(0, \\sigma)\n",
    "\\end{align}\n",
    "\n",
    "2. The residuals should be **homoscedastic** with respect to the predicted values, $\\hat{Y}$. Homoscedastic residuals have constant variance, $\\sigma$ with respect to the predicted values. This criteria applies to any form of regression model. If this is not the case, we say that the residuals are **heteroscedastic**, with variance changing with respect to the predicted values. In other words, the variance is a function of the predicted values, $\\sigma(x_i) = f(x_i)$. A model with heteroscedastic residuals will have a better fit for small predicted values than large predicted values, or vice versa. We can write a model for hetroscedastic residuals as: \n",
    "\n",
    "$$\\epsilon_i = N(0, f(x_i))$$\n",
    "\n",
    "As an example, consider a situation where the variance of the residuals grows exponentially with the value of the predictor.    \n",
    "$$\\epsilon_i = N(0, e^{x_i})$$    \n",
    "\n",
    "In this case, the residuals will be strongly heteroscedastic. This outcome should alert any modeler to the fact that a better model is required. A logarithmic transformation of the variable is one possibility here. \n",
    "\n",
    "To start our analysis of residuals, execute the code in the cell below to compute residuals for the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add residuals to pandas dataframe\n",
    "sim_data['resids'] = np.subtract(sim_data.predicted, sim_data.y)\n",
    "\n",
    "# View head of data frame\n",
    "sim_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can measure the dispersion of the residuals as a measure of regression performance. The metric is root mean square error or RMSE, which is very close to the standard deviation:\n",
    "\n",
    "$$RMSE = \\sqrt{\\frac{ \\Sigma^n_{i-1} (y_i - \\hat{y_i})^2}{n}}= \\sqrt{\\frac{SSR}{n}}$$\n",
    "\n",
    "Since the residuals should be distributed as $N(0, \\sigma)$, we should also determine if the mean of the residuals is approximately 0. \n",
    "\n",
    "Execute the code in the cell below to compute and display the mean and RMSE for the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The mean of the residuals = {0:4.3f}  RMSE = {1:4.3f}'.format(np.mean(sim_data.resids), np.std(sim_data.resids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a reasonable value for RMSE considering the scale of the lable, $\\{0,11\\}$. Further, the mean of the residuals is effectively 0 as required by our model. \n",
    "\n",
    "Next, we need to determine if the distribution of the residuals is approximately Normal. Execute the code in the cell below to plot a histogram and Q-Q Normal plot of the residuals.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_resid_dist(resids):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n",
    "    ## Plot a histogram\n",
    "    sns.histplot(resids, bins=20, kde=True, ax=ax[0])\n",
    "    ax[0].set_title('Histogram of residuals')\n",
    "    ax[0].set_xlabel('Residual values')\n",
    "    ## Plot the Q-Q Normal plot\n",
    "    ss.probplot(resids, plot = ax[1])\n",
    "    ax[1].set_title('Q-Q Normal plot of residuals')\n",
    "    plt.show()\n",
    "\n",
    "plot_resid_dist(sim_data.resids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residuals are reasonably close to Normally distributed.   \n",
    "\n",
    "Now, we must explore if the residuals are homoscedastic. A robust diagnostic for homoscedasticy is to create a **residual plot** with the residuals on the vertical axis plotted against the predicted values on the horizontal axis. Execute the code in the cell below to display an example.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_plot(df, predicted='predicted', resids='resids'):\n",
    "    fig,ax = plt.subplots(figsize=(12,5))\n",
    "    RMSE = np.std(df.loc[:,resids])\n",
    "    sns.scatterplot(x=predicted, y=resids, data=df, ax=ax)\n",
    "    ax.axhline(0.0, color='red', linewidth=1.0)\n",
    "    ax.axhline(2.0*RMSE, color='red', linestyle='dashed', linewidth=1.0)\n",
    "    ax.axhline(-2.0*RMSE, color='red', linestyle='dashed', linewidth=1.0)\n",
    "    ax.set_title('PLot of residuals vs. predicted')\n",
    "    ax.set_xlabel('Predicted values')\n",
    "    ax.set_ylabel('Residuals')\n",
    "    plt.show()\n",
    "    \n",
    "residual_plot(sim_data)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These residuals look quite homoscedastic. In summary, the fit of the linear model looks quite good.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 19-1:** You will now construct a single regression model for the price of automobiles given the weight. Using statistical terminology we can stay we want to model the dependent variable, price, by the independent variable, or explanatory variable, weight. Now, follow these steps:   \n",
    "> 1. Execute the code in the cell below to load the required dataset and center (zero mean) the independent variables you will work with.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_price = pd.read_csv('../data//AutoPricesClean.csv')\n",
    "for col,new_col in zip(['curb_weight','engine_size'],['curb_weight_centered','engine_size_centered']):\n",
    "    auto_price[new_col] = auto_price.loc[:,col] - np.mean(auto_price.loc[:,col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 2. Next, in the cell below create and execute the code to define and fit the OLS model and print the summary. Use a formula `'price ~ curb_weight_centered'`. Name your model `auto_ols`.  \n",
    "> 3. Print the summary of the model.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 4. Next, you will examine the statistical properties, distribution and homoscedacity of the residuals as following:      \n",
    ">  - Print the mean and the root mean square of the residuals.     \n",
    ">  - Plot the histogram and QQ-Normal plot of the residuals.   \n",
    ">  - Create a residual plot of the residuals vs. the predicted values.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_price['predicted'] = auto_ols.predict(auto_price.curb_weight_centered)\n",
    "auto_price['resids'] = np.subtract(auto_price.predicted, auto_price.price)\n",
    "\n",
    "## Your code below\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now answer these questions:      \n",
    "> 1. Are the residuals 0 centered and what does this tell you about the model?       \n",
    "> 2. Are these residuals Normally distributed and if not what does this tell you about the model?    \n",
    "> 3. Are the residuals homoscedastic or heteroscedastic and why do you reach this conclusion?      \n",
    "> 4. Considering the adjusted R squared, the F statistic and the Omnibus statistic, does this model have explainatry power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**      \n",
    "> 1.           \n",
    "> 2.          \n",
    "> 3.         \n",
    "> 4.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 19-2:** You might be able to improve the model by a transformation of the response variable. To find out, do the following:       \n",
    "> 1. Using the formula `'np.log(price) ~ curb_weight_centered` compute a new model, named `auto_ols_log`.  \n",
    "> 2. Print the summary of the model.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Put your code below\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 3. Next, you will examine the statistical properties, distribution and homoscedacity of the residuals of the new model as following:  \n",
    ">  - Compute the predicted values from the model and place them in a column in the data frame.    \n",
    ">  - Using the predicted values, compute the residuals and place them in a column in the data frame. \n",
    ">  - Print the mean and the root mean square of the residuals.     \n",
    ">  - Plot the histogram and QQ-Normal plot of the residuals.   \n",
    ">  - Create a residual plot of the residuals vs. the predicted values.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code below\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 4. Next execute the code in the cell below to plot the observed values and the curve of the predicted values.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_price['predicted_transformed'] = np.exp(auto_price['predicted'])\n",
    "## Plot the data and predicted values\n",
    "fig,ax = plt.subplots(figsize=(8,8))\n",
    "ax = sns.lineplot(x='curb_weight', y='predicted_transformed', data=auto_price, color='red');\n",
    "sns.scatterplot(x='curb_weight', y='price', data=auto_price, ax=ax);\n",
    "ax.set_ylabel('Vehicle Weight');\n",
    "ax.set_xlabel('Vehicle Price');\n",
    "ax.set_title('Vehicle price vs. weight with fitted model line');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Examine these results and consider the answers to the following questions:\n",
    "> 1. Compare the $R^2$, adjusted $R^2$ and the F-statistic between the original model and the model using the log transformed price. What does the difference say about the change in variance explained?             \n",
    "> 2. How has the mean of the residuals and RMSE changed between the models?     \n",
    "> 3. Do the residuals of the log price model appear to be approximately Normally distributed with zero mean?     \n",
    "> 4. Are the residuals of the log transformed price model homoscedastic?     \n",
    "> 5. What do the foregoing observations tell you about the fit of the log transformed price model?        \n",
    "> 6. How can you interpret the model coefficients in terms of the transformed response variable?     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**      \n",
    "> 1.     \n",
    "> 2.          \n",
    "> 3.     \n",
    "> 4.      \n",
    "> 5.     \n",
    "> 6.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Linear regressions are not just for straight lines\n",
    "\n",
    "A linear model is linear in its coefficients. But, that does not mean we are limited to straight lines, **a common misconception**.  A partial list of functions which can be included in a linear model includes:\n",
    "\n",
    "- Polynomials, but beware of polynomials of degree 3 or above.\n",
    "- Splines and smoothing kernels.\n",
    "- trigonometric functions.\n",
    "- Logarithmic and exponential functions.\n",
    "- Interaction terms, which are the product of feature values. For example, the two-way interaction of var1 and var2. In the R modeling language, interactions are specified as $var1:var2$. You can express the using both varibles plus the interaction as, $var1*var2 = var1 + var2 + var1:var2$.  \n",
    "\n",
    "You have already worked a first example which employed a logrithmic transformation. We will continue with another example. \n",
    "\n",
    "### Another Example\n",
    "\n",
    "To clarify these concepts, let's look at an example. The code in the cell below computes a data set with a polynomial trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramters of generated data\n",
    "n_points = 50\n",
    "x_start, x_end = 0, 10\n",
    "y_sd = 4.0\n",
    "incpt = 5.0\n",
    "\n",
    "# Generate data columns\n",
    "nr.seed(474747)\n",
    "x_data = np.linspace(x_start, x_end, n_points) # The x values\n",
    "y_error = np.random.normal(loc=0, scale=y_sd, size=n_points) # The Normally distributed noise\n",
    "y_data = x_data + 0.6 * np.square(x_data) + y_error + incpt # The y values including an intercept\n",
    "\n",
    "# Put data in dataframe\n",
    "sim_data_poly = pd.DataFrame({'x':x_data, 'y':y_data})\n",
    "\n",
    "sim_data_poly.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below computes an simple ordinary least squares regression model for the data just created. Exectute this code and note the values of the slope and intecept coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the regresson model and fit it to the data\n",
    "ols_model = smf.ols(formula = 'y ~ x', data=sim_data_poly).fit()\n",
    "\n",
    "# Add predicted to pandas dataframe\n",
    "sim_data_poly['predicted'] = ols_model.predict(sim_data_poly.x)\n",
    "\n",
    "print(ols_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a feel for how well this model fits the data, execute the code in the cell below to plot the regression line and the observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lineplot(x='x', y='predicted', data=sim_data_poly, color='red')\n",
    "sns.scatterplot(x='x', y='y', data=sim_data_poly, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit is not too bad, but you can see that the straight line is far from perfect.   \n",
    "\n",
    "Next, execute the code in the cell below to compute and plot the residuals.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_data_poly['resids'] = np.subtract(sim_data_poly.predicted, sim_data_poly.y)\n",
    "\n",
    "print('The mean of the residuals = {0:4.3f} median of residual = {1:4.3f}'.format(np.mean(auto_price.resids),np.mean(auto_price.resids)))\n",
    "print('RMSE = {0:4.3f}'.format(np.std(auto_price.resids)))\n",
    "\n",
    "plot_resid_dist(sim_data_poly.resids)\n",
    "residual_plot(sim_data_poly)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the residuals is far from Normal. Further, it is clear from the residiual plot that the residuals are from homoscedastic. The residuals are negative at the extremes of the predicted values and positive in the middle.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Polynomial Features\n",
    "\n",
    "Given the poor fit of the first model, it is time to try something else. In this case, we will try using polynomial terms. \n",
    "\n",
    "First, execute the code in the cell below to create a centered dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_data_poly['x_centered'] = sim_data_poly['x'] - np.mean(sim_data_poly['x'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Scaling Data\n",
    "\n",
    "When performing regression with multiple numeric features, you will often need to **scale the data**.  Scaling data is important not just for regression, but most other machine learning models. Some reasons to scale regression data include:\n",
    "\n",
    "- Scaling prevents features with a large numerical range from overwhelming features with small numerical values. Numerical range is not an indicator of feature importance!\n",
    "- Related to the above point, you may encounter convergence problems with many machine learning algorithms if the numeric range of the features is different. \n",
    "\n",
    "There are several possible approaches to scaling data:\n",
    " - Scale the features or independent variables. This is the most common practice.\n",
    " - Scale the label or dependent variable, which is rarely done. It is generally better practice to perform a transform the distribution to be closer to Normal. \n",
    " - Scale both features and label. Rarely required.  \n",
    " \n",
    "In this case, we take advantage of the fact that `statsmodels.formula.api` models perform the scaling. For example if our model formula is `y ~ x + I(x**2)`, statsmodels scales the features, $x$ and $x^2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 19-3:** In the cell below create and execute code to do the following:\n",
    "> 1. Define and fit an OLS model using the x_centered and x_centered squared features. This model will be linear in the coefficients of these features. The model formula uses the `I()` operator to create the higher order terms of the model. For example, the $x^2$ term is defined as `I(x**2)`. Name your model `ols_model_poly`.\n",
    "> 2. Print the summary of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 3. Now, you will evaluate the behavior of the residuals. In the cell below, create and execute code to do the following:    \n",
    ">   - Compute the predicted values from the model and save these in the predicted column of the data frame.   \n",
    ">   - Compute the residuals and save these in the resids column of the data frame. \n",
    ">   - Compute and print the mean and RMSE of the residuals. \n",
    ">   - PLot the histogram and Q-Q Normal plot of the residuals. \n",
    ">   - Display the residual plot the residuals vs. the predicted values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Put your code below\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 4. To get a feel for the model fit create and execute code to plot the regression line (predicted values) and the observations in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add your code below\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Examine the plots and consider the answers to these questions:\n",
    "> 1. Are all model coefficients significant or only some and why?    \n",
    "> 2. Is the polynomial model significant based on the F-statistic and why?    \n",
    "> 3. How have $R^2$, adjusted $R^2$ and RMSE changed with the addition of the higher order feature and what does this mean in terms of the fit of the model?      \n",
    "> 4. Do the residuals appear to be approximately Normally distributed with zero mean?\n",
    "> 5. Are the residuals approximately homoscedastic?    \n",
    "> 6. Now, consider the entire evaluation of the model. Is the model a good fit to these data?   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1.       \n",
    "> 2.     \n",
    "> 3.    \n",
    "> 4.     \n",
    "> 5.      \n",
    "> 6.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leverage and Cook's Distance\n",
    "\n",
    "Up to now, we have only looked at regression models with Normally distributed noise or errors. But, in the real world there are errors and outliers in data. These errors and outliers can have greater or lesser effect, depending on how extreme they are and their placement with respect to the other data. \n",
    "\n",
    "You can imagine a regression line as a lever. Outliers that occur near the ends of the lever will have a greater influence, all other factors being equal. \n",
    "\n",
    "One way to measure influence of a data point is **Cook's distance**, introduced by Dennis Cook in 1977. The influence for the ith data point can be computed as:\n",
    "\n",
    "$$D_i = \\frac{\\Sigma_{j=1}^n (\\hat{Y}_j - \\hat{Y}_{j(i)})^2}{n (p+1)\\hat{\\sigma^2}}$$\n",
    "where,\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{Y}_{j} &= the\\ jth\\ prediction\\ computed\\ with\\ all\\ observations\\\\\n",
    "\\hat{Y}_{j(i)} &= the\\ jth\\ prediction\\ computed\\ without\\ the\\ ith\\ observation\\\\\n",
    "p &= number\\ of\\ parameters\\\\\n",
    "\\hat{\\sigma^2} &= empirical\\ variance\\ estimate\\ of\\ the\\ complete\\ set\\ of\\ residuals\\\\\n",
    "n &= number\\ of\\ data\\ points\n",
    "\\end{align}\n",
    "\n",
    "In effect, cooks distance compares the mean squared difference between predicted values with and without a given data point. This calculation is updated excluding each observation one at a time. Thus, Cook's distance is a resampling method. Therefore, computing Cook's distance can be computationally intensive for large datasets. Typically, Cook's distance is measured in units of standard deviation.\n",
    "\n",
    "A great many methods have been developed to analyze leverage and influence. The Statsmodels packages uses a version of Cook's distance that does not adjust for the number of samples and uses a different DoF correction:    \n",
    "\n",
    "$$D_i = \\frac{\\Sigma_{j=1}^n (\\hat{Y}_j - \\hat{Y}_{j(i)})^2}{p\\ \\hat{\\sigma^2}}$$\n",
    "\n",
    "Let's make these concepts concrete with an example. To start, execute the code in the cell below to create a data set with an outlier added and plot the observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data frame with single outlier\n",
    "outliers = pd.DataFrame({'x':[0.0], 'y':[15], 'predicted':[0.0], 'resids':[0.0]})\n",
    "sim_data_outlier = pd.concat([outliers,sim_data]).reset_index().drop('index', axis=1)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(7,7))\n",
    "ax = sns.scatterplot(x='x', y='y', data=sim_data_outlier, ax=ax)\n",
    "_=ax.set_title('Observations with outlier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below does the following:\n",
    "- Defines and fits an OLS model to the data set with the outlier. \n",
    "- Computes predictions based on this data set.\n",
    "- Plots the regression line along with the observations. \n",
    "\n",
    "Execute this code and examine the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Define the regresson model and fit it to the data\n",
    "ols_model_outlier = smf.ols(formula = 'y ~ x', data=sim_data_outlier).fit()\n",
    "\n",
    "print(ols_model_outlier.summary())\n",
    "\n",
    "# Add predicted to pandas dataframe\n",
    "sim_data_outlier['predicted'] = ols_model_outlier.predict(sim_data_outlier['x'])\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(7,7))\n",
    "ax = sns.lineplot(x='x', y='predicted', data=sim_data_outlier, color='red', ax=ax)\n",
    "sns.scatterplot(x='x', y='y', data=sim_data_outlier, ax=ax)\n",
    "_=ax.set_title('Regression line with influence of outlier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the slope of the line has changed because of the outlier. This is an example of effect of a **high leverage outlier**.  \n",
    "\n",
    "You can create an **influence plot**, which shows the influence and leverage of the observations vs. the standardized residuals. The [statsmodels.graphics.regressionplots.influence_plot](https://www.statsmodels.org/stable/generated/statsmodels.graphics.regressionplots.influence_plot.html) displays Cook's distance as a measure of influence. Execute the code in the cell below to display the influence plot for the OLS model of the data with the outlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(8,8))\n",
    "_=influence_plot(ols_model_outlier, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting plot shows variance standardized residuals on the vertical axis vs. Cook's distance leverage on the horizontal axis. There is one observation, the outlier or point 0, which has significant influence on the regression model. This observation has a large standardized residual and significant leverage. Other observations have high leverage, but small residuals. \n",
    "\n",
    "Stats models also provides the [statsmodels.graphics.regressionplots.plot_regress_exog](https://www.statsmodels.org/stable/generated/statsmodels.graphics.regressionplots.plot_regress_exog.html) plotting method which displays a series of **partial regression plots**. A partial regression plot displays various measures of the regression model fit vs. a selected exogenous variable or feature. Execute the code in the cell below to see an example of these plots and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 8))\n",
    "_=plot_regress_exog(ols_model_outlier, 'x', fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These plots provide several views of the fit of the model to the observations. The effect of the outlier is quite evident in these plots.   \n",
    "\n",
    "Next, execute the code in the cell below to display the characteristics of the distribution of the residuals.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_data_outlier['resids'] = np.subtract(sim_data_outlier.predicted, sim_data_outlier.y)\n",
    "\n",
    "## Your code below\n",
    "print('The mean of the residuals = {0:4.3f}  RMSE = {1:4.3f}'.format(np.mean(sim_data_outlier.resids), np.std(sim_data_outlier.resids)))\n",
    "\n",
    "plot_resid_dist(sim_data_outlier.resids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outlier clearly has an effect on the residuals.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 19-4:** We can try a more complex model for the price of the autos by introducing an additional explanatory variable. Additionally, we introduce an interaction term between the two explanatory variables. The interaction term being the product of the two variables. To perform this analysis do the following steps:     \n",
    "> 1. Define a model named `auto_ols_2` with model formula `np.log(price) ~ curb_weight_centered * engine_size_centered`.   \n",
    "> 2. Compute the predicted values and store them in the `predicted` column of the data frame.   \n",
    "> 3. Compute the residuals and store them in the `resids` column of the data frame.   \n",
    "> 4. Print the mean and the RMSE of the residuals.    \n",
    "> 5. Display the distribution plots of the residuals.   \n",
    "> 6. Display a plot of residuals vs. predicted values.   \n",
    "> 7. Finally, display the influence plot for the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Your code goes here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now answer these questions:    \n",
    "> 1. Examine the summary of the coefficient values. Are all the model coefficient significant and how can you tell?        \n",
    "> 2. Compare the RMSE, $R^2$, and adjusted $R^2$ of this model with the first model using a single independent variable. What does the change tell you about the relative model fits?   \n",
    "> 3. Compare the F-statistic of this model with the first model using a single independent variable. Keeping in mind the change of degrees of freedom with the increase in model parameters, is the change in F-statistic reasonable and why?       \n",
    "> 4. Are the Residuals approximately Normal and homoscendasitc?           \n",
    "> 5. Examine the leverage and residuals on the influence plot. Are there any high-leverage residuals shown on this plot?        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**     \n",
    "> 1.     \n",
    "> 2.     \n",
    "> 3.      \n",
    "> 4.     \n",
    "> 5.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 19-5:** It is often worth exploring the data observations that do not fit the model well. In this case you will print the make, price, curb_weight and engine_size columns for the observations with the five highest influence. You can identify these observations as the ones with the largest marker size on the influence plot you created for Exercise 19-4.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What characteristics can you see that account for the high influence of these observations.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:** Four of these cars are in the luxury-sports car category and thus are high priced and have large engines relative to their weight. The fifth car is a low priced economy car with a small engine relative to its weight. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robust Regression Algorithms  \n",
    "\n",
    "So far we have only been working with linear ordinary least squares (OLS). The response of a linear model to an observation value is linear, by definition. Consequently, there will be a large proportionate response to any outlier. This effect can strongly influence the training of linear models, as you have seen from the OLS model previously computed and evaluated. This effect is defined by the **influence function**.         \n",
    "\n",
    "**Robust regression** models limit the influence of outliers when training a model. There have been many types of robust regression models that been developed over many years. These models are based on different types of influence functions.    \n",
    "\n",
    "There are trade-offs with using robust regression. OLS models are generally **unbiased minimum variance estimators**. However, by limiting the influence function a robust model is no longer unbiased or minimum variance. In technical terms we say that the robust models are not **efficient**. \n",
    "\n",
    "Given the aforementioned trade-off, one seeks to design robust estimators with the following properties:   \n",
    "1. The estimator can be linear, and therefore unbiased, near the expected value\n",
    "2. Limit the influence of outliers by using a nonlinear influence function beyond the linear region.   \n",
    "3. The linear response near the expected value allows use of MLE methods. \n",
    "\n",
    "The median estimator is a well-known to be robust, but is not idea since it does not have all of these properties. This situation can be understood by examining the figure below.   \n",
    "\n",
    "<img src=\"../images/MeanMedianInfluence.png\" alt=\"Drawing\" style=\"width:350px; height:250px\"/>\n",
    "<center>Influence functions of linear and median estimators: Credit, Hampel, et.al. Robust Statistics, 1986</center>  \n",
    "\n",
    "Comparing these influence functions we can see: \n",
    "1. The influence function of the linear model is labeled T=mean. This function is linear and unbounded, meaning that the influence of an extreme outlier is also unbounded. The linear behavior and continuous derivatives of the influence function mean that this estimator is both an efficient and minimum variance MLE.   \n",
    "2. The median estimator is far from linear and has discontinuous derivatives about the estimated value. These properties lead to high variance from median estimates.  \n",
    "\n",
    "For a state of the art discussion of robust estimators see [Maronna, et.al., Robust Statistics, Theory and Methods (with R), Second Edition, 2019, Wiley](https://www.wiley.com/en-us/Robust+Statistics:+Theory+and+Methods+(with+R),+2nd+Edition-p-9781119214687).  \n",
    "\n",
    "### Alpha-trimming       \n",
    "\n",
    "A simple robust estimator is called **alpha trimming**. The idea is to simply delete the observations with the largest positive and negative residuals. OLS regression is trained using the remaining values.       \n",
    "\n",
    "The fraction of the observations removed in this manner is known as $\\alpha$. Typically, $\\alpha/2$ of the positive and $\\alpha/2$ of the negative residuals are deleted. Typical value of $\\alpha$ are 10%, 20% or 30%, but most any value greater than 50% can be used.     \n",
    "\n",
    "The larger the value of $\\alpha$ the more robust the estimator is. We say that the larger value of $\\alpha$ gives the estimator a higher **breakdown point**. The breakdown point is equal to  $\\alpha$ for this estimator.        \n",
    "\n",
    "Examples of influence functions for alpha trimmed estimators are shown in the figure below.  \n",
    "\n",
    "<img src=\"../images/AlphaTrimming.png\" alt=\"Drawing\" style=\"width:350px; height:250px\"/>\n",
    "<center>Influence functions of linear and alpha-trimmed estimators: Credit, Hampel, et.al. Robust Statistics, 1986</center>    \n",
    "    \n",
    "Examine the figure above and notice the following.     \n",
    "1. As stated before, the influence function of the linear model, labeled $\\bar{X}$, is linear and unbounded.   \n",
    "2. The influence function of alpha-trimmed estimators are locally linear, but then bounded. This bounding limits the influence of outliers to a constant. The slopes of the linear portion of the alpha-trimmed estimators are not the same as the linear estimator. The combination of the increased slope and bounding of the influence function results in bias and the estimator no longer being minimum variance. Estimators with higher values of alpha have more bias.            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huber estimators    \n",
    "\n",
    "Peter Huber (1964, 1984) proposed an estimator that possess the minimum variance properties near the expected value but limits the influence of outliers to a constant. In contrast to alpha-trimming, the Huber estimator does not discard observations, but rather places a hard limit on their influence. The influence function of the Huber estimator can be seen in the figure below.     \n",
    "\n",
    "<img src=\"../images/Huber.png\" alt=\"Drawing\" style=\"width:350px; height:250px\"/>\n",
    "<center>Influence function of Huber estimator: Credit, Hampel, et.al. Robust Statistics, 1986</center>  \n",
    "\n",
    "Examining the above figure, we can see some key properties of the Huber estimator:   \n",
    "1. The influence function is linear near the mean but constant away from the mean.   \n",
    "   - The **hinge point** is at $\\pm t * MAD$, where $MAD =$ **median absolute deviation**.    \n",
    "   - Robustness and bias increases as $t$ decreases. \n",
    "2. Huber estimator is low bias.      \n",
    "   - The estimator is unbiased for samples near the point estimate.   \n",
    "   - There is constant influence away from the point estimate, for outliers beyond the hinge points, $\\pm t$.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M-estimators   \n",
    "\n",
    "A large family of robust estimators are known as **M-estimators**. M-estimators attempt to improve on the properties of the Huber estimator by slowly decreasing the influence of extreme outliers toward 0. Whereas a Huber estimator has constant influence beyond the hinge point, an M-estimator will down-weight extreme outliers. Quite a number of M-estimators have been proposed.    \n",
    "\n",
    "All M-estimators require somewhat complicated selection of hyperparameter values. The Huber estimator has a single hyperparameter, the hinge point. M-estimators often have two or more hyperparameters.   \n",
    "\n",
    "A fairly simple example of an M-estimator is the Tukey biweight estimator (Tukey 1981). The influence function of the biweight estimator can be seen in the figure below.   \n",
    "\n",
    "<img src=\"../images/TukeysBiweight.png\" alt=\"Drawing\" style=\"width:400px; height:250px\"/>\n",
    "<center>Influence function of the Tukey biweight estimator: Credit, Hampel, et.al. Robust Statistics, 1986</center>  \n",
    "\n",
    "\n",
    "\n",
    "The biweight estimator uses a smooth influence function that is locally linear around the point estimate, eventually tapering to 0. The point at which the biweight becomes zero is the single hyperparameter, $r$. Both Robustness and bias increase with decreasing $r$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 19-6:** You will now create and evaluate a robust regression model for the `sim_data_outlier` dataset by the following steps:       \n",
    "> 1. Define a robust regression model using [statsmodels.formula.api.rlm](https://www.statsmodels.org/dev/generated/statsmodels.formula.api.rlm.html) using the formula `\"y ~ x\"`,with the argument `M=sm.robust.norms.HuberT()` and name your model `ols_model_huber`.  \n",
    "> 2. Print the summary of the model.     \n",
    "> 3. Compute the predictions and save them in a `predicted_huber` column of the data frame.    \n",
    "> 4. Compute the residuals and save them in a `residuals_huber` column of the data frame.   \n",
    "> 5. Display the distribution plots of the residuals.        \n",
    "> 6. Plot the residuals vs. the predicted values. \n",
    "> 7. Display a plot with the following:    \n",
    ">  - A scatter plot of the x-y values of the `sim_data_outlier` dataset.     \n",
    ">  - A line showing the predicted values from the robust regression.    \n",
    ">  - A line, using a different line type and color, showing the predicted values from the linear model.    \n",
    ">  - Include a legend on your plot.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Put your code below\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Answer the following questions:      \n",
    "> 1. Are there significant differences between the coefficients of the Huber and OLS model and which ones are closer to the true values, less the outlier?          \n",
    "> 2. Are the residuals close to Normally distributed and homoscedastic, except the outlier?         \n",
    "> 3. From the scatter plot, does in appear that the Huber estimator model fits the bulk of the data better?     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**       \n",
    "> 1.     \n",
    "> 2.     \n",
    "> 3.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copyright 2017, 2018, 2019, 2020, 2021, 2022, 2023 Stephen F Elston. All rights reserved. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
